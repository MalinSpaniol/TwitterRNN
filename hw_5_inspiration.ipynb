{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries from the tokenized text\n",
    "def create_dicts_from_tokenized_text(tokenized_text,vocabulary_size):\n",
    "    words_and_count = Counter(tokenized_text).most_common(vocabulary_size - 1)\n",
    "    print(words_and_count)\n",
    "    word2id = {word: word_id for word_id, (word, _) in enumerate(words_and_count, 1)}\n",
    "    word2id[\"_UNKNOWN_\"] = 0\n",
    "    id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "    return word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_CDU1.encode('utf-8').strip()\n",
    "\n",
    "#with open(\"tweets_CDU1.txt\", 'rb') as f:\n",
    "\n",
    "#    text_char = f.read()\n",
    "\n",
    "#text_char = open(\"tweets_CDU1.txt\",'rb').read()\n",
    "#text_char = open(\"all_files1.txt\",'r').read()\n",
    "\n",
    "with open(\"tweets.txt\", encoding=\"utf8\", errors='ignore') as f:\n",
    "    text_char = f.read()\n",
    "    \n",
    "    \n",
    "text_char = re.sub(r\"@\", \"\", text_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'', 'Boris', 'Bayreuther', '#EU', 'Outfit:', 'Faltencheck:', '\"Meistersinger\"', 'realitaetsfern.', 'Steuererhoehungen', '18.', 'Stimmt', 'solchen', 'nicht', 'Rennen', 'Langsam', 'Palmers', 'Manderla', 'intolerant', 'noch', 'Nachtkritik', 'Jahr', 'beschliet', 'meiste,', 'bettercallaxel', 'Familien', 'gestellt', 'BMI_Bund:', '#PDF', 'Direktkandidatin', 'Laut', 'Die', 'kamen', 'kahrs', 'Kommt', 'So', 'jreichelt:', ':', 'Platz', 'Hauptsache,', 'spdde', '&amp;', 'Hier', 'jetzt', 'Tourismusministerium', 'Wachsende', 'die', 'grundsaetzlich', 'Prozesse', 'wieder', 'Nuernberger', '#EU-ICT-Karte', 'das', 'Was', 'allen', 'Fachkraefte-#Zuwanderung', 'Erfolg', 'YouTube', 'wird', 'wiegelmann:', 'gemobbt', 'CDU:', 'neue', 'einer', 'Zukunft', 'den', 'Kloeckner', 'Gesinnung', 'via', 'aus\"', 'Schwarz-Gruen', 'nicht!', '2017', 'bereits', '#Europabarometer:', 'Mindestlohnerhoehung', 'werden', 'ein', 'geprueft.', 'r#BTW17', 'Gisela', 'Text', 'nicht.][][][Damnits_Kathy', 'CDU,', 'unser', '.PeterWeissMdB:', 'Urlauber-Rekord:', 'III', 'mal', 'stellt', 'Unsere', 'haben', 'der', 'koennen', 'Wagners', 'rponline:', 'getan?', 'Sie', 'Koeln', 'starker', '\"Wir', 'Festspiele:', 'vor.Bald', 'drei', 'sicher', 'primitiv.', '#BayreutherFestspiele:', 'Viel', 'und', 'ein:', 'auf', 'vermeldet', 'Nisalahe:', 'Antraege', 'Junge_Union:', 'Potomaker:', 'XING_de', 'zu', 'schlieen', 'Besucher,', 'Macht', 'MDRAktuell:', 'Ein', 'an', '\\\\n', 'Meistersinger', 'um', 'cducsubt:', '#Pflege\"', 'wir', 'Spruechen,', 'positiv', 'Deutschen', ',', 'Bundeskabinett', 'nach', 'diesem', 'Links=Selbstgerecht,', 'ueber', 'linken', 'WP', 'Abschied', 'greift', 'Mehrheit', 'schaetzt', 'im', 'meister_schafft\\\\nbei', 'rot-gruenen', 'hoffentlich', '#Kuba', 'fuer', 'helfen\"', '3.', 'welt,', 'Lohn', 'in', 'Millionen', 'Julia', 'mit', '\"Mehr', 'Spa', 'Wahlkrampf.', 'Iss', 'Europaeer', 'von', 'erinnern', '#Download'}\n",
      "vocab size: 166\n",
      "text lenght: 198\n",
      "198\n"
     ]
    }
   ],
   "source": [
    "# Generate the vocabulary.\n",
    "#vocab = list(set(text_char))\n",
    "#vocab_size = len(vocab)\n",
    "#print(vocab)\n",
    "#print(\"vocab size: {}\".format(vocab_size))\n",
    "#\n",
    "#text_len = len(text_char)\n",
    "#print(\"text lenght: {}\".format(text_len))\n",
    "#print(text_char)\n",
    "\n",
    "\n",
    "# 1.st way to do it\n",
    "tokenized_text = list(text_char.split(\" \"))\n",
    "vocab = set(tokenized_text)\n",
    "\n",
    "# 2nd way to do it - tokenized es, aber dann sind einzelne Buchstaben und Satzzeichen weg\n",
    "#tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#tokenized_text = list(tokenizer.tokenize(text_char))\n",
    "#vocab = set(tokenized_text)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "vocab\n",
    "print(vocab)\n",
    "print(\"vocab size: {}\".format(vocab_size))\n",
    "\n",
    "text_len = len(tokenized_text)\n",
    "print(\"text lenght: {}\".format(text_len))\n",
    "print(text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to switch between the indices of the characters and the characters themselves.\n",
    "#char_to_idx = { ch:i for i,ch in enumerate(vocab) }\n",
    "#idx_to_char = { i:ch for i,ch in enumerate(vocab) }\n",
    "\n",
    "# Translate the text to indices.\n",
    "#text_idx = [char_to_idx[c] for c in text_char]\n",
    "\n",
    "# create dictionaires from tokenized text with a vocabulary size of 10000\n",
    "#word2id, id2word = create_dicts_from_tokenized_text(vocab, 1000000)\n",
    "\n",
    "# create dictionaries\n",
    "word_to_id = { ch:i for i, ch in enumerate(vocab)}\n",
    "id_to_word = {i:ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "# Translate the text to indices.\n",
    "text_idx = [word_to_id[c] for c in tokenized_text]\n",
    "#word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the lenght of sequences we want to train on.\n",
    "seq_len = 5\n",
    "\n",
    "# Generate the dataset.\n",
    "input_data = []\n",
    "target_data = []\n",
    "for i in range(text_len-seq_len):\n",
    "    input_data.append(text_idx[i:i+seq_len])\n",
    "    target_data.append(text_idx[i+1:i+seq_len+1])\n",
    "    \n",
    "#input_data = np.cast(input_data, dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Create the TensorFlow dataset.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_data,target_data))\n",
    "\n",
    "dataset\n",
    "#dataset = tf.reshape (dataset,[193,5])\n",
    "#added to make float 32 and get correct shape\n",
    "#dataset = tf.cast(dataset, dtype = tf.float32)\n",
    "\n",
    "# We do not train on batches or shuffle the dataset.\n",
    "iterator = tf.data.Iterator.from_structure(dataset.output_types,dataset.output_shapes)\n",
    "iterator_init_op = iterator.make_initializer(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input from the generator.\n",
    "next_batch = iterator.get_next()\n",
    "input_data = next_batch[0]\n",
    "target_data = next_batch[1]\n",
    "\n",
    "# Create one hot tensors.\n",
    "input_one_hot = tf.one_hot(input_data, depth=vocab_size)\n",
    "target_one_hot = tf.one_hot(target_data, depth=vocab_size)\n",
    "\n",
    "# Initialize the placeholder for the hidden state.\n",
    "hidden_size = 100\n",
    "init_hs = tf.placeholder(shape=[1, hidden_size], dtype=tf.float32)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'RNN/add_2:0' shape=(1, 166) dtype=float32>, <tf.Tensor 'RNN/add_5:0' shape=(1, 166) dtype=float32>, <tf.Tensor 'RNN/add_8:0' shape=(1, 166) dtype=float32>, <tf.Tensor 'RNN/add_11:0' shape=(1, 166) dtype=float32>, <tf.Tensor 'RNN/add_14:0' shape=(1, 166) dtype=float32>]\n",
      "Tensor(\"concat:0\", shape=(5, 166), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Build the model.\n",
    "with tf.variable_scope(\"RNN\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    \n",
    "    # Set hidden state.\n",
    "    hs_t = init_hs\n",
    "    # Initialize list to save the hidden states and outputs of the sequence.\n",
    "    hs = []\n",
    "    ys = []\n",
    "    \n",
    "    \n",
    "    # Initialize all weights and biases.\n",
    "    initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "    Wxh = tf.get_variable(\"Wxh\", [vocab_size, hidden_size], initializer=initializer)\n",
    "    Whh = tf.get_variable(\"Whh\", [hidden_size, hidden_size], initializer=initializer)\n",
    "    Why = tf.get_variable(\"Why\", [hidden_size, vocab_size], initializer=initializer)\n",
    "    bh  = tf.get_variable(\"bh\", [hidden_size], initializer=initializer)\n",
    "    by = tf.get_variable(\"by\", [vocab_size], initializer=initializer)\n",
    "    \n",
    "    # Unfold the RNN for as many steps as our sequence is long.\n",
    "    for t in range(seq_len):\n",
    "                \n",
    "        # Read out the ith input and the ith target character\n",
    "        xs_t = input_one_hot[t,:]\n",
    "        xs_t = tf.expand_dims(xs_t, axis=0)\n",
    "        ts_t = target_one_hot[t,:]\n",
    "        \n",
    "        # Compute the new hidden state.\n",
    "        hs_t = tf.tanh(tf.matmul(xs_t, Wxh) + tf.matmul(hs_t, Whh) + bh)\n",
    "        # Compute the new output.\n",
    "        ys_t = tf.matmul(hs_t, Why) + by\n",
    "        # Store hidden state and output.\n",
    "        hs.append(hs_t)\n",
    "        ys.append(ys_t)\n",
    "        \n",
    "# The RNN is done. \n",
    "# Save the hidden state for feeding it to the next sub sequence.\n",
    "hs_remember = hs[0]\n",
    "print(ys)\n",
    "# Compute the softmax of the very last prediction for sampling.\n",
    "output_softmax = tf.nn.softmax(ys[-1])\n",
    "# Compute the loss of all the outputs.\n",
    "outputs = tf.concat(ys, axis=0)\n",
    "print(outputs)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=target_one_hot, logits=outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer.\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "training_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 4.881181\n",
      "----\n",
      " um Bayreuther CDU an Besucher gestellt der Meistersinger Zukunft gruenen Besucher Wagners Boris Urlauber der solchen Kloeckner Europabarometer stellt der \n",
      "----\n",
      "\n",
      "Epoch: 1, Loss: 5.546035\n",
      "----\n",
      " Meistersinger Tourismusministerium auf meiste primitiv Boris Wir zu So Palmers mal allen via Wahlkrampf nach BayreutherFestspiele Fachkraefte primitiv um Direktkandidatin \n",
      "----\n",
      "\n",
      "Epoch: 2, Loss: 4.752392\n",
      "----\n",
      " r Pflege bettercallaxel die gestellt BMI_Bund Fachkraefte Jahr jreichelt mit kahrs Download primitiv hoffentlich Spa Kloeckner Bundeskabinett amp helfen Selbstgerecht \n",
      "----\n",
      "\n",
      "Epoch: 3, Loss: 5.130781\n",
      "----\n",
      " Mindestlohnerhoehung wird Viel Antraege zu meiste Erfolg gruenen das Sie sicher Selbstgerecht der grundsaetzlich 3 diesem das kahrs Boris werden \n",
      "----\n",
      "\n",
      "Epoch: 4, Loss: 3.605276\n",
      "----\n",
      " vermeldet Meistersinger in Bald Zuwanderung noch jreichelt allen drei kahrs 2017 nicht positiv Gisela der So Manderla 2017 intolerant Prozesse \n",
      "----\n",
      "\n",
      "Epoch: 5, Loss: 2.978689\n",
      "----\n",
      " rot Gisela jreichelt cducsubt spdde ein die Palmers Steuererhoehungen PDF Antraege Macht schlieen Schwarz Millionen Tourismusministerium Urlauber Download erinnern Millionen \n",
      "----\n",
      "\n",
      "Epoch: 6, Loss: 2.444080\n",
      "----\n",
      " Damnits_Kathy um Unsere Erfolg 2017 3 Urlauber Bayreuther Rennen Die Julia erinnern MDRAktuell werden positiv jreichelt Macht kahrs nach via \n",
      "----\n",
      "\n",
      "Epoch: 7, Loss: 1.340504\n",
      "----\n",
      " hoffentlich Damnits_Kathy Besucher kamen Antraege kahrs grundsaetzlich Antraege CDU jreichelt der werden Antraege Kommt jetzt Damnits_Kathy allen nicht jreichelt jreichelt \n",
      "----\n",
      "\n",
      "Epoch: 8, Loss: 1.314999\n",
      "----\n",
      " Steuererhoehungen das geprueft kahrs Selbstgerecht Gisela Gisela diesem Jahr Gisela via in Potomaker PDF Erfolg positiv Laut jreichelt in gestellt \n",
      "----\n",
      "\n",
      "Epoch: 9, Loss: 0.560832\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(10):\n",
    "    \n",
    "        # Set time step counter.\n",
    "        t = 0\n",
    "        # Load the dataset into the iterator.\n",
    "        sess.run(iterator_init_op)\n",
    "        hs_remember_val = np.zeros([1,hidden_size])\n",
    "\n",
    "        # Go through the dataset until its empty.\n",
    "        while True:\n",
    "            try:\n",
    "                # If we are in the first time step intialize the hidden state with zeros.\n",
    "                #if t == 0:\n",
    "                #    hs_remember_val = np.zeros([1,hidden_size])\n",
    "                \n",
    "                # Feed in the last hidden state.\n",
    "                # Read out the loss value for printing.\n",
    "                # Read out the hidden state for the next forward step.\n",
    "                # Do the training step.\n",
    "                hs_remember_val, loss_val, _ = sess.run([hs_remember, loss, training_step], feed_dict={init_hs: hs_remember_val})\n",
    "                \n",
    "                # Increment the time step.\n",
    "                t += 1\n",
    "\n",
    "            # Stop if iterator is empty.\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "                \n",
    "          \n",
    "        # After each epoch we print the loss value.\n",
    "        print(\"Epoch: {}, Loss: {:f}\".format(epoch, loss_val))\n",
    "        \n",
    "        # The main validation procedure we will use is sampling a text from our model to see how good we \n",
    "        # approximate the original dataset.\n",
    "        \n",
    "        # How many words would we like to sample?\n",
    "        sample_len = 20\n",
    "        \n",
    "        # Get a random starting sequence from our training dataset.\n",
    "        start_idx = random.randint(0, len(text_idx) - seq_len)\n",
    "        seq_idx = text_idx[start_idx:start_idx + seq_len]      \n",
    "      \n",
    "        # List to store the characters sampled by our model.\n",
    "        sample_seq_idx        = []\n",
    "        sample_hs_remember_val = np.zeros([1,hidden_size])\n",
    "\n",
    "        # Sample as many characters as we would like.\n",
    "        for n in range(sample_len):\n",
    "            \n",
    "            # To feed the starting sequence into our model we first need to put it into a dataset.\n",
    "            # As we do not compare anything here we need some fake target values.\n",
    "            fake_target = np.zeros([1,5], dtype=np.int32)\n",
    "            sample_dataset = tf.data.Dataset.from_tensor_slices(([seq_idx], fake_target))\n",
    "            # Load this dataset into the iterator.\n",
    "            sess.run(iterator.make_initializer(sample_dataset))\n",
    "    \n",
    "            # Now we need to read out two things. The softmax output for sampling a character and the hidden state to\n",
    "            # feed it in again.\n",
    "            sample_output_softmax_val, sample_hs_remember_val = sess.run([output_softmax, hs_remember],\n",
    "                                                                       feed_dict={init_hs: sample_hs_remember_val})\n",
    "\n",
    "            \n",
    "            \n",
    "            # Sample a character from the softmax output distribution and append it.\n",
    "            sample = np.random.choice(range(vocab_size), p=sample_output_softmax_val.ravel())\n",
    "            sample_seq_idx.append(sample)\n",
    "            # Update the start sequence for sampling the next character\n",
    "            seq_idx = seq_idx[1:] + [sample]\n",
    "        \n",
    "      \n",
    "        # Print sample.\n",
    "        sample_txt = ' '.join(idx_to_char[idx] for idx in sample_seq_idx)\n",
    "        print('----\\n %s \\n----\\n' % (sample_txt,))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch: 0, Loss: 2.288059\n",
    "----\n",
    " ea  diruun FeSnier uannnn wgrnte?ytilftrer L,eeerdine H afubate  ieanngeer Fangla #nninaiuenernienel tean?inenge  Ninmene e ?uerergrernFer erreen  min v?l ?mnuegnn,, agiman agernFeninfn Wier slgnFAgtn \n",
    "----\n",
    "\n",
    "Epoch: 1, Loss: 1.804944\n",
    "----\n",
    " ei Fmmerel?Fer Fel lerera?ae? Fafer Fepe e: hil  ?r lis Fium escee  macaen HFelienhender aer n?u get en uerlh? Hienietserdun wet n? Hie  unstr Fie n?enck ser lel I? efer Ficier  aahen Falin,gtaacv l?. \n",
    "----\n",
    "\n",
    "Epoch: 2, Loss: 2.072745\n",
    "----\n",
    " ae. :Ftderhinuer EindRn dit 18.uW: W8sden N8?sin wH\"m18 S8r 18.k[? 18 1W7r 8.s1M i8-ger 18.ker WPrdtn der 18ndWP 18 1g  ina nt Werf1n 18r 18sWir WAnder U8b 1? S8aderume  ien Mamen wer 18o WP SHKann un \n",
    "----\n",
    "\n",
    "Epoch: 3, Loss: 1.868061\n",
    "----\n",
    " 1asi: Si de, r.wind ! b laim n Sin 1n miit leachhe @ungnn fanvin Fusd rerelt. Lilg:ntVwel fn H ?]emst eermen n=timd gr Eiruchen 1et WichenKong 1in nn .wisgen Jamikn angener serflir gi fer wen wicaFuet \n",
    "----\n",
    "\n",
    "Epoch: 4, Loss: 1.925632\n",
    "----\n",
    " h Wung fie Gasmimie dier Fimita nairkal KunsFe La Wi= ala awin SWat? ?eer WWfmer des vesReb der Ee hhrFar werd h amir! Wami D? eats ESmenerulala Get nacafa ier iet wir Fr des naenrg \"Werl ieamar 1? do \n",
    "----\n",
    "\n",
    "Epoch: 5, Loss: 1.861674\n",
    "----\n",
    " eN unein. ESxest ir  imitentu \" amPstuenefn in  Mimat Fouruss 1rh rober Saeinstdun  ToWet Fer wisimil: nichkinintlieities Wain din  wum.mit une hner  ind sbeneen\" Hichtiabknhuab rnner upg under uei 1M \n",
    "----\n",
    "\n",
    "Epoch: 6, Loss: 1.768264\n",
    "----\n",
    " s hisker Weicht!Kn ua der Witiriamter urhink !gurhun in uan ietin. na ukineu dierer Wch un Kintser Ffehlabee Fim kind read: Muirkr dah mabt oirtin @18m ra \"chergeen wilin wamit, mchtkele  womin. Wectk \n",
    "----\n",
    "\n",
    "Epoch: 7, Loss: 2.081336\n",
    "----\n",
    " mine hrmen volchuteiln hntoraman ruenkbarhis  laelterenmenhr Lin der int #Prung litBun Tohdutiunt Sicher oanfu hamn  vien n ndra ned.l mahnermn dnern delren der ies n aahtichen  @Paft an d,  anchera ? \n",
    "----\n",
    "\n",
    "Epoch: 8, Loss: 1.534197\n",
    "----\n",
    " : Eut her-hnbSt beiftieit. nn in der 18nen tin. itsen des ir it unt in meimder viintg her ien Haurtoeflngennnin lia Knen ie  Eeimititn mesak gem geien .Minnchn. ierei der Anber .in  isuele: vis Aberne \n",
    "----\n",
    "\n",
    "Epoch: 9, Loss: 1.435708\n",
    "----\n",
    " ichind hirzenmieg 1n. ialan Sen 18sahtin anfinganiungen Pialen dinbanfanren grhier Faha det Pbe: Fifohangebfa de  Pifherunn nalin axt  einialanen gne geamin in ae? ant  Patknbeht!  minitit! nacht! mhn \n",
    "----\n",
    "\n",
    "Epoch: 10, Loss: 1.713243\n",
    "----\n",
    " nd di ii gn gie gr Gitgn Snes der  Koen gnent Sne tien gn Stiman Sber Tubit Fichun Fatin gn de  Eir inukn Sie ent 18 Histheneng get ubtden der @Kubt gnereter ger 18 viag !amir Fn: 18s 18xin geun Stiut \n",
    "----\n",
    "\n",
    "Epoch: 11, Loss: 1.762047\n",
    "----\n",
    " ah kramenthear Ei .ita hreit in de in gesiet ri: den Eitensaenes der 1n. nir uets  @naetstist Fasin  Jan Ban Bar unter  Sosers Sch neum urneist rit nite vafh hoe de: Eirant Gis nien det 18a Wei ges ai \n",
    "----\n",
    "\n",
    "Epoch: 12, Loss: 1.860812\n",
    "----\n",
    " : Licheroch rech efing  Wachlar Finueterhon din mising miegest riutkander sichlea nerit aaaut: Lahrer ain soerneitit Siteistonig der mia telitia hia uagte Waffer  in ieefne: Wif ii. wnegentenbet uns r \n",
    "----\n",
    "\n",
    "Epoch: 13, Loss: 1.659803\n",
    "----\n",
    " !riam nam nia #in susala ienen Sesachamtin Spa 18a Wer 18czt?enge Seiomeinismmin ubderbagteim Landen Wnseroenden Spiunschichen Sit lisker ium nicht! Wa  u=ier ALcaklien #inir der abamth  @fo guschem   \n",
    "----\n",
    "\n",
    "Epoch: 14, Loss: 1.692321\n",
    "----\n",
    "  salaen nicht! Washekoscauneilae deun Wa Istampd &al :ir Tan itsmir wir in gn ,in smen wisien Ger, geaun ioschl: Wauchampin \" Sonbet Loer Eamist am win fin, naen rielkan en hWiche: Gisirien G_Walaliw  \n",
    "----\n",
    "\n",
    "Epoch: 15, Loss: 1.697626\n",
    "----\n",
    " it  Wa aefo1n  ir prden  @xorberSerfth Wio n urusmet Siramg inkan uschan in all: , sich n Hay winden insin sie s Lailt Wae w Wen Fber Feithvcan uthnn? @kermutchr! inser JarZunmet  Wireut vir musoeg ge \n",
    "----\n",
    "\n",
    "Epoch: 16, Loss: 1.456457\n",
    "----\n",
    "  Snsierichennin Famlernongey un Sssuninken Was veerlitier lihkerennun  @Uuft via Eu Ler spder und n unis_ber upa vor iul neun mit sotsnien gerllatkn dni neun bat Toung Nmehketamit ver belahe: Sif #Pel \n",
    "----\n",
    "\n",
    "Epoch: 17, Loss: 1.696989\n",
    "----\n",
    " n an hesMeb richen Ln der 18. War uechliabtr Wahn Manelt nninn Wae wer Pelentsmit enten?uen ners Srin geruntsneber Tbir warde gept pieler ner er Katzl der wicheitinndn  Stunleen ei \\n. nichun din sien \n",
    "----\n",
    "\n",
    "Epoch: 18, Loss: 1.697832\n",
    "----\n",
    " neu gin der CDU prsahgn ber Ctanern &e Minhln destener han uncheiresllienen Wi  spoeng: Wa @werder Ansing aer stsitieign an wer 18. Kut palahn get nis Tamps weloAnund  un ierschennine Ien #EU pitachta \n",
    "----\n",
    "\n",
    "Epoch: 19, Loss: 1.027246\n",
    "----\n",
    "   @belmet. @Patkra heften Men nelalit meubntin eis koalit vorbel #EDlobd alier nnsil? IIstllit korrint#MDer samilia -we @netalit der uther wicher Gndere Mochknen Anelich !estt hieln Facher Lahinbelman \n",
    "----\n",
    "\n",
    "Epoch: 20, Loss: 1.252708\n",
    "----\n",
    "  3. mubalocs Rei Kut  ver @beltger beronn: Eun drunner \\erdeu Sieket ninnen Sp wer nie rualien nech  @iekoue  nen @nalien ber 1v  ubsfehrnin nerlntsm Ein uen der und n.en ger in der 18. WP fuer Famili \n",
    "----\n",
    "\n",
    "Epoch: 21, Loss: 1.300344\n",
    "----\n",
    " er  Laelt Mun Fastharennunner  Haur Sacds Lan wie nie Zufu ber Sr Gv.a @weisivra fst-Wir Eafongenut v. inder werder ut. @beld: ._scha,  @m iex WP fu mithe @eimnt, WP Fniste, La @CMoentl. @CIunber Wr i \n",
    "----\n",
    "\n",
    "Epoch: 22, Loss: 0.755693\n",
    "----\n",
    " nuer Wahh  Sir Piatker ner kar hn her 18. WP fuer Familien get n? Hier Jalliaben hn  Jalilaen ner naninnen  Haur  aninse den grianen Lis Rettlabekvea Pitzelfe  wir ,ia v Jungiraninien nnd nia Bor Baxi \n",
    "----\n",
    "\n",
    "Epoch: 23, Loss: 0.948605\n",
    "----\n",
    " Soe tlandenzn die sersift nicher intillaelt acheneln: #iuemttundingeru gnder  goaut alitkraubm sech nMan Poeer  aunisamalngnm liane! Win ltaup? virlited viatkneun Fanden Sen eest hex  voeift mamesta h \n",
    "----\n",
    "\n",
    "Epoch: 24, Loss: 0.528845\n",
    "----\n",
    " n ein Spr dia her inier Felllinnen niu nia @bed vie dre \\nienen Speustinich nei  Mauschachenhn Mehn ber C8. WP Nuer Faxth bef nicher Grselern. Linge Unescha, unen Steumit debent  Eullaekanimnt.mibet F \n",
    "----\n",
    "\n",
    "Epoch: 25, Loss: 0.313106\n",
    "----\n",
    "  ban iet mit stauer  neisteall: Wachker Tmxtk aeln Mahrerchaettnecht. IIs  ardel gneln: #Eulier ietzn die en Famein: mh wersunder winden Wahllt inn lier besterinet Jichn: Way kraubbt holfo @iesenbenen \n",
    "----\n",
    "\n",
    "Epoch: 26, Loss: 0.589241\n",
    "----\n",
    "   Haup Peftenttin @BayreutherFan grieftla ber ueu Eer Eirien Hah  Spe \" d. werdin der mekel Fellin. . uellnen @beut voaungem ber WPufter Die Zuku @ir @bem Aberein dir Doun Sta muthlaekn  in ztampr: Wa \n",
    "----\n",
    "\n",
    "Epoch: 27, Loss: 0.691363\n",
    "----\n",
    " lnenten miehstonninnen SpaupreatherFestPahte seia @wa @wim vi .#PfpTech nounn in olauelt,  @fotstbeln: Eifett  Eif @bereit  @DFebst. KoNn Pr werFelmant: Wa wktie, win die melt, wind gn eistersinthn ge \n",
    "----\n",
    "\n",
    "Epoch: 28, Loss: 0.436280\n",
    "----\n",
    " ampf. Viel Soungis dri Eg eichenenng dit motan? in wira #EUrputa EFante Wachsernbetserein: WPrfuen Spr Erannen Sie stieiti \\efstech erung nSteltrelachend n I@ @wanis Fan wirden Antliexeln d der deu go \n",
    "----\n",
    "\n",
    "Epoch: 29, Loss: 0.125220\n",
    "----\n",
    " ht, intolerant und realit enefn: #Ein er ueber decht!olnu @PDF betze \" .achtkritik z-aruen Spd er uefungo Stiuer Bandersabei ho bestler .ahtse \"ie stemiti ieisten das meita  winden Spruechen, Eafomnte \n",
    "----\n",
    "\n",
    "Epoch: 30, Loss: 0.320884\n",
    "----\n",
    " in: Wagners Meisterfieh necht ,ists meinge \"Mehstericht, in er Fanlfun\" Bu, dreungen Meister_ csin nin ge Min unhen : Wacherhen\" voeIs \"Weristachengen Ser pemalt: Wagn roer stsun Pichkn en Ba wititich \n",
    "----\n",
    "\n",
    "Epoch: 31, Loss: 0.323988\n",
    "----\n",
    " s kamen Linden Win uba kra nKen  isschrh, Ber sehte \" inabs ,am komtie getser  Manistir enssmebet  iomet Micht tin dtr innde  Wagisamae un Wir utiele: Wagners Meistersinger. Wachse sin der inn er eise \n",
    "----\n",
    "\n",
    "Epoch: 32, Loss: 0.119709\n",
    "----\n",
    " YouTube  Felllehe: .ts Kalp: Lind voan His Ka lahen  inalita Fistcher 1usinn nis Laen Ste #bp @bechel\" vin.d rissch 3. Prbschanesta hettnecs  Vien ho-IIst voriftkoeln Sie me  @spn.btin ge d: FastRebe: \n",
    "----\n",
    "\n",
    "Epoch: 33, Loss: 0.611523\n",
    "----\n",
    " solchelnen des halennun un  iestoer #EU maspre foed n an ze scheldte #Euwinmeld dellagn Mochteriuthnet Beristsanisneckon Gesch jbst die dt  seutsv. .Bun Si den Absaeite  Mehts=Selbstgerecht. irhoue an \n",
    "----\n",
    "\n",
    "Epoch: 34, Loss: 0.088004\n",
    "----\n",
    " poberfutBunderaninaen ber CDU, Kochtiange schlitameffing masistenobn gerin Poen Struer Sin en Spr achtemet an zierhenden Sinberh, ube Zukanber Seu Eut Twer_salahn an wierheis MIIItioWa  htienfe, Min S \n",
    "----\n",
    "\n",
    "Epoch: 35, Loss: 0.086176\n",
    "----\n",
    " erden Giser Fusaklat. #Landen Sie Zukunft der #Pflege\" 1isken in dre foe, Fastkerhin dis lickt! WPst Mellnben get uedt Im Plehtern Nauhukampie Kabmielin din erher Fahtkramit v. Platz 3.  @Potomaker: S \n",
    "----\n",
    "\n",
    "Epoch: 36, Loss: 0.058115\n",
    "----\n",
    " Palmenze #DF gese bie hopfn Min Zprungb Soe msicher in @simelt Mach  Nisana Mam nerNach @ochun Sp besta den Sie pre1ft Wichtneis Mahr: Longsammibt  @mollterW initsto fett voelland n inlchen in der 18. \n",
    "----\n",
    "\n",
    "Epoch: 37, Loss: 0.369997\n",
    "----\n",
    " stericher#bn dis heimift re matze den  itspreinedt der Eir pieln: Warahtititit reit vneln Botinit Meldetsmefitit die Zukungesttimititit  @CNtsie dit teritl amt prueg  iosec Mind: Babli aener Uulangem  \n",
    "----\n",
    "\n",
    "Epoch: 38, Loss: 0.585235\n",
    "----\n",
    " ldetgerechnieittenlinktern in dee 18. @Kut din #Bun @rUellten Fest dia @winden Sin pr ies  berolltaneint die mat nbeder Wahn Spelmit-soben ein ,  @MDU, \"oelle \"Meht mosmiber  Hauftercch @opoelt der  M \n",
    "----\n",
    "\n",
    "Epoch: 39, Loss: 0.169559\n",
    "----\n",
    " n niangt Soeistsacher nicht.][][][@Damnits_Kathy Kommt hoffentlich noch @outelit @kotomere din Abtun? Hier utsucb St ber liakgr Spibstsamber nicht. Geschr, .@mitititin. das meianbe gerouchenent\\n @XI- \n",
    "----\n",
    "\n",
    "Epoch: 40, Loss: 0.067973\n",
    "----\n",
    " skned genichtfele voerueghuefin din det 18. WP fehr Familien getan? Hier unser Faltenchechenren @neuftin vie hophenten grichtinieio Stimit Biriet Menden Sie premptin @bes Minkt=enn: Uamokchln: sthder  \n",
    "----\n",
    "\n",
    "Epoch: 41, Loss: 0.016879\n",
    "----\n",
    " gemobbt  @CLinkonenn nach ,oen Steue ma Despchoed  uRa @welmekn @bein undin dmelaber Dicht, Wichkraenn: Wagn Sie Zutu grimitin.ht uestlahn get lianner Sin Zukunft der #Pfleetin diestP mierhon,e @tpsie \n",
    "----\n",
    "\n",
    "Epoch: 42, Loss: 0.110818\n",
    "----\n",
    " -#Zuwander @rounger Pfoenlingen gerschlla Jurd geanbe  Dinntan ers achte Jun isstolit &rmpfte,  Gestlahechen, Erroeh  aelftel @,edet Wagnn sanfe gesol? Hier unser. Wan sacherten di  #Pfteg \"Wich aopan \n",
    "----\n",
    "\n",
    "Epoch: 43, Loss: 0.090779\n",
    "----\n",
    " ueu Ber seroeber Tusrhtinit  Deromitaloe: #EUrpalahn Min nir Eur Toueltinite  @Grung aete  @Jihkvt iet  @Jelster_in der Bun nich a ieite deis Mehrielt Bin der #BU rerlanken ner Bra @welttinden Uisere  \n",
    "----\n",
    "\n",
    "Epoch: 44, Loss: 0.052000\n",
    "----\n",
    " cht! Wasfhen Sieterbei spd det licht: Was haben wir in der 18. WP fuer Familien getan? Hier unser Faltencoebs hreungen gest alla her\" in @er ier lach noenget Way sMalitaestin @voungen geuch @Nisabahen \n",
    "----\n",
    "\n",
    "Epoch: 45, Loss: 0.058419\n",
    "----\n",
    " Mechlitneckon in der 18. WP fuer Familien getan? Hier unsen inedte #Kunge_Unien Sprueg nicht  iestsamin viruet alletzu deu #Bayreuther Festspieln: WP wercsechla zensellandes Reistech noch @Nindet Minn \n",
    "----\n",
    "\n",
    "Epoch: 46, Loss: 0.047460\n",
    "----\n",
    " -ICT-Kalt, Klitkv. Ssn all kande ges PrlinndnBscht, iet Uaue @relllia @kotoulden St  @rFestanin vie ssacherstlaers Solieltaen Abschlmelnun Sie aef zsalihaen riniestechre Mun dveutlun dis kraestldasen  \n",
    "----\n",
    "\n",
    "Epoch: 47, Loss: 0.093918\n",
    "----\n",
    " ht. [][][@Damnits_Kathy Kommt hoffentlich noch @Nisalahe: Links=Selbstgerecht, intole: #EU poamt#MLinitaen in @XImGBset Minag wers Mahrl Sollt aMDe: \"Mehn sundin  iaut MaBtkunden ei Prho Mahe toolle z \n",
    "----\n",
    "\n",
    "Epoch: 48, Loss: 0.069884\n",
    "----\n",
    " l d, wGamelt der gruen em toengen niotan Sie grlin enetent darinaehe: Unsere Nacht! ies hamel eindin @romatdestUn Spa prpittiser En des Rerachtopote #wander ue weruen Sperpen dpr mal warde  @sdRe sich \n",
    "----\n",
    "\n",
    "Epoch: 49, Loss: 0.419565\n",
    "----\n",
    " ut in der WP iner Fbelteliti  @Juhsen dt ued riam wor in en gelintor, ag Nubel gn anberges\"bel @kahr PUhr: Stersemalban Bellag  Gesebali: Spimmt rochtaeisterlingen dir lichla in der 18. WP fuer Famili \n",
    "----\n",
    "\n",
    "Epoch: 50, Loss: 0.161123\n",
    "----\n",
    " en getan? Hier unser Faltencanbexer ningnndt un @iche: eindin ei Pin: n Hiemen Steuet Wachtkriunhnit urhelt Wachets \" in der 18. WP fuer Familien getan? Hier unser Faltknnen stadin Ourf mma @welt,  @M \n",
    "----\n",
    "\n",
    "Epoch: 51, Loss: 0.012235\n",
    "----\n",
    " Michers @Pdestlohnn Sie net lerN Muisaen itsung lich foung EU-grsam?its nicht, intolerant und realitaetsfern. Ein stanker Eur laat ar ParNanhen inhrn an Muca @windin Sie krimitiv. Lin s=Selbstgerecht, \n",
    "----\n",
    "\n",
    "Epoch: 52, Loss: 0.012508\n",
    "----\n",
    " e sunger Winks=Spibst!ergellPhn der Eurolaxen \"poung  Schmmt  Jurisgealft\\n @Nahne: Ste melmalbrn hesfs malmn Sit mit ,ereich nosnner schersinien  DoPrstacheft Bor. acherg Mingen dir wer werden Sie kr \n",
    "----\n",
    "\n",
    "Epoch: 53, Loss: 0.067895\n",
    "----\n",
    " ramitkrichen unsonge_Et der #Plmober der #Pa wer mesalien hr ange ge fut-mriant: So berhlin Ba @spersinger  Hauptsabed . WPlaetzl #ruwan wis 1berger unnen ein Sparner unnentst ung mit uer loanger  @Pu \n",
    "----\n",
    "\n",
    "Epoch: 54, Loss: 0.028655\n",
    "----\n",
    " ubauber Er acheGr: Lochun  Europitanden #EUromitariet erscheiss\"Win knechech nochschlibis nier nmut  ietalit Eeristlabickatsnerhen un en wierium Per karSpaber wirdin Siemprtimicht! intkrrien wir en ei \n",
    "----\n",
    "\n",
    "Epoch: 55, Loss: 0.015111\n",
    "----\n",
    " bbt  vie @ndcs dolle der Europaees Mahtsem icht.][][@Panes La, kander Fastsilinn: Buy spaelft dot  Pachen Spruechektsfftiettin Mend gemolit eit scheft der llaktsuchenesscbelt Mahp: Lochker Tlxa @warde \n",
    "----\n",
    "\n",
    "Epoch: 56, Loss: 0.027101\n",
    "----\n",
    " en Gesinnung  Stimmt nicht! War hacheneStem Eundets Maer  @miretschlieer Steuer Wahlkaben spdpieitiun: #Europabarometersunger wirdeg mobeue sindisschaer  Lahpr Sch hechteuesen @nech nossteesMabkrach n \n",
    "----\n",
    "\n",
    "Epoch: 57, Loss: 0.004124\n",
    "----\n",
    " sache, einer wird geunger MindestSchen um Platz 3.  @Potomaker: Schwarz-Gruen im rot-gruenen Outfit: Julia Kloeckner stellt Boris Palmen Siener Wahkkramit voche \" poutstiall: #Eurosabalter  Haukskm :  \n",
    "----\n",
    "\n",
    "Epoch: 58, Loss: 0.052840\n",
    "----\n",
    " cht Tibek hrinin gneis Lohh  ellin zlinnen gersungein der CDUNschen in di  @spon. Ginen sichln eis  aun suchlltz_ichen in der mesalite  Lunks=Seln t Wic kaben wirhin anDin vomwinhen g sien wer in zlie \n",
    "----\n",
    "\n",
    "Epoch: 59, Loss: 0.019101\n",
    "----\n",
    " nks=Selbs grpriei n allhen 1in der 18.  vit sold: und der liann: Sie primitin zi   LanissUch neG sin ier laats=malg  ia De zlin Bum Preraub in der 18. WP fuer Familien getan? Hier unser Faltenchecd: , \n",
    "----\n",
    "\n",
    "Epoch: 60, Loss: 0.011379\n",
    "----\n",
    " ,  @BMI_Bund: Fachkraefte-#Zuwange Wag srernen nicher 18 Plahnbet  Wif @spodet richtam.licher\" n ausunn  Dahpr Lohnn mieher Lings LWin Machktae \"it die  @Mehrsuecklehn ut ies nbche: Wastsaben Men @Bal \n",
    "----\n",
    "\n",
    "Epoch: 61, Loss: 0.020064\n",
    "----\n",
    " ommt hrfoeht gelingers Pohrsemaln dir Antolitan Sin interesegen git #Europabar werden Steuets\"Meistersinger\" eomn: Sof ues_Bames Langs=Sffen Sie zen Spr aensem linken Geter Faltenchecknko Diemabesten  \n",
    "----\n",
    "\n",
    "Epoch: 62, Loss: 0.011175\n",
    "----\n",
    " gerecht, intol? Haer unser Faltencheck: #Diwiet dis Bomel werrie Mebs haeferun  iestobereussinisien gntaleraet Minde  Gerellan?  @Clinnon Bis mela Berrelche \" @rungelmaner Wagners Meistersinger  Haup# \n",
    "----\n",
    "\n",
    "Epoch: 63, Loss: 0.300900\n",
    "----\n",
    " rger Prozesch @Nind  vindenmabirgelt , Falliet nber @Hiuet rich : Son achln aeisteder Unserbete zo he  @Pllnieit Sor Barer FahDkramit vo zu P poerg lach iosn Ser Biser wird gemob,  #EF @Mdin om werden \n",
    "----\n",
    "\n",
    "Epoch: 64, Loss: 0.008051\n",
    "----\n",
    "  inssseer, MdSr umben des 18. WP fuer Familien getan? Hier #EU positiv gelm Pamen in din #PDF @checsubt: .@Peter Fehsanden Uuthkaeck Hoankander Wihlkan nn.nnsunge_Uninn zit pomen eit I8. WP nuer Fahp  \n",
    "----\n",
    "\n",
    "Epoch: 65, Loss: 0.009194\n",
    "--------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
