{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Get all tweets by members of a certain party\n",
    "    input:  Party = list of the screennames of the members of a political party\n",
    "            all_tweets = a file with all tweets\n",
    "    output: all tweets by either of the screennames provided\n",
    "'''\n",
    "def get_tweets_by(party, all_tweets):\n",
    "    tweets = []\n",
    "    # go through all tweets\n",
    "    for i in range(len(all_tweets)):\n",
    "        # go through all members of the party\n",
    "        for member in party:\n",
    "            # if the screenname of the current tweet is in the list -> append\n",
    "            if (all_tweets[i]['user']['screen_name'] == member):\n",
    "                tweets.append(all_tweets[i]['text'])\n",
    "    return tweets\n",
    "\n",
    "''' Remove unwanted symbols from the text\n",
    "    input:  a string file\n",
    "    output: the string without the unwanted symbols\n",
    "'''\n",
    "def clean_text(text):\n",
    "    # remove URl links\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # remove @screenames\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)\n",
    "    # remove the indicator for a retweet \"RT\"\n",
    "    text = re.sub(r\"RT\", \"\", text)\n",
    "    #text = re.sub(r\"\\'\", \"\", text)\n",
    "    # remove emojis etc\n",
    "    text = re.sub(r\"üá©üá™\", \"\", text)\n",
    "    text = re.sub(r\"üòÅ\", \"\", text)\n",
    "    text = re.sub(r\"üòÄ\", \"\", text)\n",
    "    text = re.sub(r\"'‚ù§'\", \"\", text)\n",
    "    text = re.sub(r\"‚ù§\", \"\", text)\n",
    "    text = re.sub(r\"üéâ\", \"\", text)\n",
    "    text = re.sub(r\"'Ô∏è'\", \"\", text)\n",
    "   \n",
    "    # replace \"Umlaute\" and √ü\n",
    "    text = re.sub(r\"√º\", \"ue\", text)\n",
    "    text = re.sub(r\"√∂\", \"oe\", text)\n",
    "    text = re.sub(r\"√§\", \"ae\", text)\n",
    "    text = re.sub(r\"√ü\", \"ss\", text)\n",
    "    # add a space before each dot to keep it as its own token\n",
    "    text = re.sub(r\"\\.\", \" .\", text)\n",
    "    # remove single symbols\n",
    "    ''.join( c for c in text if  c not in '[,],/,:,&,_,1,2,3,4,5,6,7,8,9,0,,' )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('followed-accounts.json') as json_file:\n",
    "    followed_accounts = json.load(json_file)\n",
    "    \n",
    "CDU = followed_accounts['CDU/CSU']\n",
    "SPD = followed_accounts['SPD']\n",
    "FDP = followed_accounts['FDP']\n",
    "LINKE = followed_accounts['Linke']\n",
    "GRUENE = followed_accounts['Gr√ºne']\n",
    "AFD = followed_accounts['AfD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'chunk1'\n",
    "# get all directories of the twitter data\n",
    "all_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.json')]\n",
    "\n",
    "data1 = []\n",
    "for i in range(100):\n",
    "    file = pd.read_json(all_files[i], orient='index')\n",
    "    data1.append(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-2ab601813b3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m101\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdata1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[0;32m    425\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    535\u001b[0m             )\n\u001b[0;32m    536\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'frame'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'series'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 652\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    877\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"index\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m             self.obj = DataFrame(\n\u001b[1;32m--> 879\u001b[1;33m                 loads(json, precise_float=self.precise_float), dtype=None).T\n\u001b[0m\u001b[0;32m    880\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'table'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m             self.obj = parse_table_schema(json,\n",
      "\u001b[1;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "for i in range(101,200):\n",
    "    file = pd.read_json(all_files[i], orient='index')\n",
    "    data1.append(file)\n",
    "# nur teile der data werden geladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200,300):\n",
    "    file = pd.read_json(all_files[i], orient='index')\n",
    "    data1.append(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-5222137fe890>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdata1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[0;32m    425\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    535\u001b[0m             )\n\u001b[0;32m    536\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'frame'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'series'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 652\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    877\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"index\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m             self.obj = DataFrame(\n\u001b[1;32m--> 879\u001b[1;33m                 loads(json, precise_float=self.precise_float), dtype=None).T\n\u001b[0m\u001b[0;32m    880\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'table'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m             self.obj = parse_table_schema(json,\n",
      "\u001b[1;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "for i in range(300,len(all_files)):\n",
    "    file = pd.read_json(all_files[i], orient='index')\n",
    "    data1.append(file)\n",
    "# nur teile der data werden geladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ein bisschen weniger, als gewollt...\n",
    "len(data1)\n",
    "# 362 of 436, gut genug..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_AFD1 = []\n",
    "for i in range(93):\n",
    "    tweets_AFD1.append(get_tweets_by(AFD, data1[i]))\n",
    "#get rid of 93\n",
    "for i in range(94,117):\n",
    "    tweets_AFD1.append(get_tweets_by(AFD, data1[i]))\n",
    "#get rid of 117\n",
    "for i in range(118,139):\n",
    "    tweets_AFD1.append(get_tweets_by(AFD, data1[i]))\n",
    "#get rid of 138\n",
    "for i in range(140,194):\n",
    "    tweets_AFD1.append(get_tweets_by(AFD, data1[i]))\n",
    "#get rid of 194\n",
    "for i in range(195,239):\n",
    "    tweets_AFD1.append(get_tweets_by(AFD, data1[i]))\n",
    "#get rid of 239\n",
    "for i in range(240,243):\n",
    "    tweets_AFD1.append(get_tweets_by(AFD, data1[i]))\n",
    "#get rid of 242 and 243\n",
    "for i in range(244,274):\n",
    "    tweets_AFD1.append(get_tweets_by(AFD, data1[i]))\n",
    "#get rid of 274\n",
    "for i in range(275,319):\n",
    "    tweets_AFD1.append(get_tweets_by(AFD, data1[i]))\n",
    "#get rid of 319\n",
    "for i in range(320,len(data1)):\n",
    "    tweets_AFD1.append(get_tweets_by(AFD, data1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_LINKE1 = []\n",
    "for i in range(93):\n",
    "    tweets_LINKE1.append(get_tweets_by(LINKE, data1[i]))\n",
    "#get rid of 93\n",
    "for i in range(94,117):\n",
    "    tweets_LINKE1.append(get_tweets_by(LINKE, data1[i]))\n",
    "#get rid of 117\n",
    "for i in range(118,139):\n",
    "    tweets_LINKE1.append(get_tweets_by(LINKE, data1[i]))\n",
    "#get rid of 138\n",
    "for i in range(140,194):\n",
    "    tweets_LINKE1.append(get_tweets_by(LINKE, data1[i]))\n",
    "#get rid of 194\n",
    "for i in range(195,239):\n",
    "    tweets_LINKE1.append(get_tweets_by(LINKE, data1[i]))\n",
    "#get rid of 239\n",
    "for i in range(240,243):\n",
    "    tweets_LINKE1.append(get_tweets_by(LINKE, data1[i]))\n",
    "#get rid of 242 and 243\n",
    "for i in range(244,274):\n",
    "    tweets_LINKE1.append(get_tweets_by(LINKE, data1[i]))\n",
    "#get rid of 274\n",
    "for i in range(275,319):\n",
    "    tweets_LINKE1.append(get_tweets_by(LINKE, data1[i]))\n",
    "#get rid of 319\n",
    "for i in range(320,len(data1)):\n",
    "    tweets_LINKE1.append(get_tweets_by(LINKE, data1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_CDU1 = []\n",
    "for i in range(93):\n",
    "    tweets_CDU1.append(get_tweets_by(CDU, data1[i]))\n",
    "#get rid of 93\n",
    "for i in range(94,117):\n",
    "    tweets_CDU1.append(get_tweets_by(CDU, data1[i]))\n",
    "#get rid of 117\n",
    "for i in range(118,139):\n",
    "    tweets_CDU1.append(get_tweets_by(CDU, data1[i]))\n",
    "#get rid of 138\n",
    "for i in range(140,194):\n",
    "    tweets_CDU1.append(get_tweets_by(CDU, data1[i]))\n",
    "#get rid of 194\n",
    "for i in range(195,239):\n",
    "    tweets_CDU1.append(get_tweets_by(CDU, data1[i]))\n",
    "#get rid of 239\n",
    "for i in range(240,243):\n",
    "    tweets_CDU1.append(get_tweets_by(CDU, data1[i]))\n",
    "#get rid of 242 and 243\n",
    "for i in range(244,274):\n",
    "    tweets_CDU1.append(get_tweets_by(CDU, data1[i]))\n",
    "#get rid of 274\n",
    "for i in range(275,319):\n",
    "    tweets_CDU1.append(get_tweets_by(CDU, data1[i]))\n",
    "#get rid of 319\n",
    "for i in range(320,len(data1)):\n",
    "    tweets_CDU1.append(get_tweets_by(CDU, data1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_FDP1 = []\n",
    "for i in range(93):\n",
    "    tweets_FDP1.append(get_tweets_by(FDP, data1[i]))\n",
    "#get rid of 93\n",
    "for i in range(94,117):\n",
    "    tweets_FDP1.append(get_tweets_by(FDP, data1[i]))\n",
    "#get rid of 117\n",
    "for i in range(118,139):\n",
    "    tweets_FDP1.append(get_tweets_by(FDP, data1[i]))\n",
    "#get rid of 138\n",
    "for i in range(140,194):\n",
    "    tweets_FDP1.append(get_tweets_by(FDP, data1[i]))\n",
    "#get rid of 194\n",
    "for i in range(195,239):\n",
    "    tweets_FDP1.append(get_tweets_by(FDP, data1[i]))\n",
    "#get rid of 239\n",
    "for i in range(240,243):\n",
    "    tweets_FDP1.append(get_tweets_by(FDP, data1[i]))\n",
    "#get rid of 242 and 243\n",
    "for i in range(244,274):\n",
    "    tweets_FDP1.append(get_tweets_by(FDP, data1[i]))\n",
    "#get rid of 274\n",
    "for i in range(275,319):\n",
    "    tweets_FDP1.append(get_tweets_by(FDP, data1[i]))\n",
    "#get rid of 319\n",
    "for i in range(320,len(data1)):\n",
    "    tweets_FDP1.append(get_tweets_by(FDP, data1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_GRUENE1 = []\n",
    "for i in range(93):\n",
    "    tweets_GRUENE1.append(get_tweets_by(GRUENE, data1[i]))\n",
    "#get rid of 93\n",
    "for i in range(94,117):\n",
    "    tweets_GRUENE1.append(get_tweets_by(GRUENE, data1[i]))\n",
    "#get rid of 117\n",
    "for i in range(118,139):\n",
    "    tweets_GRUENE1.append(get_tweets_by(GRUENE, data1[i]))\n",
    "#get rid of 138\n",
    "for i in range(140,194):\n",
    "    tweets_GRUENE1.append(get_tweets_by(GRUENE, data1[i]))\n",
    "#get rid of 194\n",
    "for i in range(195,239):\n",
    "    tweets_GRUENE1.append(get_tweets_by(GRUENE, data1[i]))\n",
    "#get rid of 239\n",
    "for i in range(240,243):\n",
    "    tweets_GRUENE1.append(get_tweets_by(GRUENE, data1[i]))\n",
    "#get rid of 242 and 243\n",
    "for i in range(244,274):\n",
    "    tweets_GRUENE1.append(get_tweets_by(GRUENE, data1[i]))\n",
    "#get rid of 274\n",
    "for i in range(275,319):\n",
    "    tweets_GRUENE1.append(get_tweets_by(GRUENE, data1[i]))\n",
    "#get rid of 319\n",
    "for i in range(320,len(data1)):\n",
    "    tweets_GRUENE1.append(get_tweets_by(GRUENE, data1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_SPD1 = []\n",
    "for i in range(93):\n",
    "    tweets_SPD1.append(get_tweets_by(SPD, data1[i]))\n",
    "#get rid of 93\n",
    "for i in range(94,117):\n",
    "    tweets_SPD1.append(get_tweets_by(SPD, data1[i]))\n",
    "#get rid of 117\n",
    "for i in range(118,139):\n",
    "    tweets_SPD1.append(get_tweets_by(SPD, data1[i]))\n",
    "#get rid of 138\n",
    "for i in range(140,194):\n",
    "    tweets_SPD1.append(get_tweets_by(SPD, data1[i]))\n",
    "#get rid of 194\n",
    "for i in range(195,239):\n",
    "    tweets_SPD1.append(get_tweets_by(SPD, data1[i]))\n",
    "#get rid of 239\n",
    "for i in range(240,243):\n",
    "    tweets_SPD1.append(get_tweets_by(SPD, data1[i]))\n",
    "#get rid of 242 and 243\n",
    "for i in range(244,274):\n",
    "    tweets_SPD1.append(get_tweets_by(SPD, data1[i]))\n",
    "#get rid of 274\n",
    "for i in range(275,319):\n",
    "    tweets_SPD1.append(get_tweets_by(SPD, data1[i]))\n",
    "#get rid of 319\n",
    "for i in range(320,len(data1)):\n",
    "    tweets_SPD1.append(get_tweets_by(SPD, data1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_AFDstr1 = ''.join(map(str,tweets_AFD1))\n",
    "clean_AFDstr1 = clean_text(tweets_AFDstr1)\n",
    "\n",
    "tweets_LINKEstr1 = ''.join(map(str,tweets_LINKE1))\n",
    "clean_LINKEstr1 = clean_text(tweets_LINKEstr1)\n",
    "\n",
    "tweets_CDUstr1 = ''.join(map(str,tweets_CDU1))\n",
    "clean_CDUstr1 = clean_text(tweets_CDUstr1)\n",
    "\n",
    "tweets_FDPstr1 = ''.join(map(str,tweets_FDP1))\n",
    "clean_FDPstr1 = clean_text(tweets_FDPstr1)\n",
    "\n",
    "tweets_SPDstr1 = ''.join(map(str,tweets_SPD1))\n",
    "clean_SPDstr1 = clean_text(tweets_SPDstr1)\n",
    "\n",
    "\n",
    "tweets_GRUENEstr1 = ''.join(map(str,tweets_GRUENE1))\n",
    "clean_GRUENEstr1 = clean_text(tweets_GRUENEstr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_AFDstr1.encode('utf-8', errors='ignore').strip()\n",
    "\n",
    "f= open(\"AFD1.txt\",\"w+\",encoding=\"utf8\", errors='ignore')     \n",
    "f.write(clean_AFDstr1)\n",
    "f.close()\n",
    "\n",
    "clean_LINKEstr1.encode('utf-8', errors='ignore').strip()\n",
    "\n",
    "f= open(\"LINKE1.txt\",\"w+\",encoding=\"utf8\", errors='ignore')     \n",
    "f.write(clean_LINKEstr1)\n",
    "f.close()\n",
    "\n",
    "clean_CDUstr1.encode('utf-8', errors='ignore').strip()\n",
    "\n",
    "f= open(\"CDU1.txt\",\"w+\",encoding=\"utf8\", errors='ignore')     \n",
    "f.write(clean_CDUstr1)\n",
    "f.close()\n",
    "\n",
    "clean_GRUENEstr1.encode('utf-8', errors='ignore').strip()\n",
    "\n",
    "f= open(\"GRUENE1.txt\",\"w+\",encoding=\"utf8\", errors='ignore')     \n",
    "f.write(clean_GRUENEstr1)\n",
    "f.close()\n",
    "\n",
    "clean_FDPstr1.encode('utf-8', errors='ignore').strip()\n",
    "\n",
    "f= open(\"FDP1.txt\",\"w+\",encoding=\"utf8\", errors='ignore')     \n",
    "f.write(clean_FDPstr1)\n",
    "f.close()\n",
    "\n",
    "clean_SPDstr1.encode('utf-8', errors='ignore').strip()\n",
    "\n",
    "f= open(\"SPD1.txt\",\"w+\",encoding=\"utf8\", errors='ignore')     \n",
    "f.write(clean_SPDstr1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the third chunk called data3\n",
    "\n",
    "path = 'chunk3'\n",
    "# get all directories of the twitter data\n",
    "all_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.json')]\n",
    "\n",
    "data3 = []\n",
    "for i in range(all_files):\n",
    "    file = pd.read_json(all_files[i], orient='index')\n",
    "    data3.append(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_AFD3 = []\n",
    "for i in range(418):\n",
    "    tweets_AFD3.append(get_tweets_by(AFD, data3[i]))\n",
    "for i in range(419,len(data3)):\n",
    "    tweets_AFD3.append(get_tweets_by(AFD, data3[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_LINKE3 = []\n",
    "for i in range(418):\n",
    "    tweets_LINKE3.append(get_tweets_by(LINKE, data3[i]))\n",
    "for i in range(419,len(data3)):\n",
    "    tweets_LINKE3.append(get_tweets_by(LINKE, data3[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_CDU3 = []\n",
    "for i in range(418):\n",
    "    tweets_CDU3.append(get_tweets_by(CDU, data3[i]))\n",
    "for i in range(419,len(data3)):\n",
    "    tweets_CDU3.append(get_tweets_by(CDU, data3[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_SPD3 = []\n",
    "for i in range(418):\n",
    "    tweets_SPD3.append(get_tweets_by(SPD, data3[i]))\n",
    "for i in range(419,len(data3)):\n",
    "    tweets_SPD3.append(get_tweets_by(SPD, data3[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_GRUENE3 = []\n",
    "for i in range(418):\n",
    "    tweets_GRUENE3.append(get_tweets_by(GRUENE, data3[i]))\n",
    "for i in range(419,len(data3)):\n",
    "    tweets_GRUENE3.append(get_tweets_by(GRUENE, data3[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_FDP3 = []\n",
    "for i in range(418):\n",
    "    tweets_FDP3.append(get_tweets_by(FDP, data3[i]))\n",
    "for i in range(419,len(data3)):\n",
    "    tweets_FDP3.append(get_tweets_by(FDP, data3[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_AFDstr3 = ''.join(map(str,tweets_AFD3))\n",
    "clean_AFDstr3 = clean_text(tweets_AFDstr3)\n",
    "\n",
    "tweets_LINKEstr3 = ''.join(map(str,tweets_LINKE3))\n",
    "clean_LINKEstr3 = clean_text(tweets_LINKEstr3)\n",
    "\n",
    "tweets_CDUstr3 = ''.join(map(str,tweets_CDU3))\n",
    "clean_CDUstr3 = clean_text(tweets_CDUstr3)\n",
    "\n",
    "tweets_FDPstr3 = ''.join(map(str,tweets_FDP3))\n",
    "clean_FDPstr3 = clean_text(tweets_FDPstr3)\n",
    "\n",
    "tweets_SPDstr3 = ''.join(map(str,tweets_SPD3))\n",
    "clean_SPDstr3 = clean_text(tweets_SPDstr3)\n",
    "\n",
    "\n",
    "tweets_GRUENEstr3 = ''.join(map(str,tweets_GRUENE3))\n",
    "clean_GRUENEstr3 = clean_text(tweets_GRUENEstr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_AFDstr3.encode('utf-8', errors='ignore').strip()\n",
    "\n",
    "f= open(\"AFD3.txt\",\"w+\",encoding=\"utf8\", errors='ignore')     \n",
    "f.write(clean_AFDstr3)\n",
    "f.close()\n",
    "\n",
    "clean_LINKEstr3.encode('utf-8', errors='ignore').strip()\n",
    "\n",
    "f= open(\"LINKE3.txt\",\"w+\",encoding=\"utf8\", errors='ignore')     \n",
    "f.write(clean_LINKEstr3)\n",
    "f.close()\n",
    "\n",
    "clean_CDUstr3.encode('utf-8', errors='ignore').strip()\n",
    "\n",
    "f= open(\"CDU3.txt\",\"w+\",encoding=\"utf8\", errors='ignore')     \n",
    "f.write(clean_CDUstr3)\n",
    "f.close()\n",
    "\n",
    "clean_GRUENEstr3.encode('utf-8', errors='ignore').strip()\n",
    "\n",
    "f= open(\"GRUENE3.txt\",\"w+\",encoding=\"utf8\", errors='ignore')     \n",
    "f.write(clean_GRUENEstr3)\n",
    "f.close()\n",
    "\n",
    "clean_FDPstr3.encode('utf-8', errors='ignore').strip()\n",
    "\n",
    "f= open(\"FDP3.txt\",\"w+\",encoding=\"utf8\", errors='ignore')     \n",
    "f.write(clean_FDPstr3)\n",
    "f.close()\n",
    "\n",
    "clean_SPDstr3.encode('utf-8', errors='ignore').strip()\n",
    "\n",
    "f= open(\"SPD3.txt\",\"w+\",encoding=\"utf8\", errors='ignore')     \n",
    "f.write(clean_SPDstr3)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
