{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Dateien, die wir runtergeladen haben. Waren ~1300 Chunks mit jeweils 1000 Tweets, die aber nicht alle von den Politikern selbst gepostet wurden, sondern teilweise auch von deren Anh√§ngern kommentiert oder geretweeted wurden. Das hei√üt wir mussten erst einmal aussortieren um an die richtigen Tweets zu kommen.\n",
    "Anschlie√üend haben wir den Text bereinigt und tokenisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiere die ben√∂tigten packages\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Diese Funktion geht durch alle Elemente von \"all_tweets\" und speichert nur die ab, die bei einem Mitglied der Partei ist\n",
    "    Input:  Party = Liste der Screen_names einer politischen Partei\n",
    "            all_tweets = eine Liste der originalen .json Dateien\n",
    "    Output: eine Liste der Tweets, die von einem der Mitglieder der Partei gepostet wurden\n",
    "'''\n",
    "def get_tweets_by(party, all_tweets):\n",
    "    # erstelle eine leere Liste um die Tweets abzuspeichern\n",
    "    tweets = []\n",
    "    # iteriere √ºber alle Tweets in der Datei\n",
    "    for i in range(len(all_tweets)):\n",
    "        # iteriere √ºber alle Mitglieder der Partei\n",
    "        for member in party:\n",
    "            # falls der aktuelle Tweets von dem aktuellen Mitglied ist: h√§nge diesen Tweet der Liste an\n",
    "            if (all_tweets[i]['user']['screen_name'] == member):\n",
    "                tweets.append(all_tweets[i]['text'])\n",
    "    return tweets\n",
    "\n",
    "''' Diese Funktion entfernt alle Zeichen, die wir nicht haben wollen aus der Textdatei. Da die Tweets zun√§chst als Listenelemente\n",
    "    aneinander gereit wurden (siehe Preprocessing), sind in der Textdatei noch sehr viele eckige Klammern. Au√üerdem wurden \n",
    "    Zeilenumbr√ºche als '/n' codiert und Tweets, die die Standardl√§nge von 140 Zeichen √ºberschritten haben, wurden abgeschnitten,\n",
    "    sodass teilweise unvollst√§ndige Worte vorkommen. Zudem gibt es noch einige weitere ungew√ºnschte Zeichen, wie URL-links etc.\n",
    "    Input:  Eine String Datei\n",
    "    Output: Die Datei ohne die unerw√ºnschten Zeichen\n",
    "'''\n",
    "def clean_text(text):\n",
    "    \n",
    "    # entfernt URL Links\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # entfernt @screen_name\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)\n",
    "    # entfernt \"RT\" f√ºr Re-Tweet\n",
    "    text = re.sub(r\"RT\", \"\", text)\n",
    "    # entfernt Sondersymbole wie Emojis\n",
    "    text = re.sub(r\"üá©üá™\", \"\", text)\n",
    "    #text = re.sub(r\"üòÅ\", \"\", text)\n",
    "    #text = re.sub(r\"üòÄ\", \"\", text)\n",
    "    #text = re.sub(r\"'‚ù§'\", \"\", text)\n",
    "    #text = re.sub(r\"‚ù§\", \"\", text)\n",
    "    #text = re.sub(r\"üéâ\", \"\", text)\n",
    "    # entfernt Sonderzeichen\n",
    "    text = re.sub(r\"'Ô∏è\", \"\", text)\n",
    "    text = re.sub(r\"‚Ä¶\", \"\", text)\n",
    "    text = re.sub(r\"\\\\n\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\n\", \"\", text)\n",
    "    text = re.sub(r\"]\\S+\", \"\", text)\n",
    "    # entfernt einzelne Symbole \n",
    "    text = ''.join( c for c in text if  c not in '[,],/,:,&,_,1,2,3,4,5,6,7,8,9,0,,' )\n",
    "\n",
    "    # ersetzt Umlaute und √ü\n",
    "    text = re.sub(r\"√º\", \"ue\", text)\n",
    "    text = re.sub(r\"√∂\", \"oe\", text)\n",
    "    text = re.sub(r\"√§\", \"ae\", text)\n",
    "    text = re.sub(r\"√ü\", \"ss\", text)\n",
    "    \n",
    "    # setzt ein Leerzeichen vor jeden Punkt, sodass er als einzelnes Symbol gesehen wird und W√∂rter mit Punkt\n",
    "    # nicht als eigenst√§ndige W√∂rter ins Vokabular eingehen\n",
    "    text = re.sub(r\"\\.\", \" .\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einlesen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teil des Datensets, welches wir runtergeladen haben, war auch eine Liste aller Twitter Accounts von Politikern\n",
    "# sortiert nach der Parteizugeh√∂rigeit.\n",
    "# Die Listen befinden sich in der Datei followed-accounts, die zun√§chst eingelesen wird\n",
    "with open('followed-accounts.json') as json_file:\n",
    "    followed_accounts = json.load(json_file)\n",
    "\n",
    "# Jetzt m√ºssen die Namen der einzelnen Accounts nur noch getrennt abgespeichert werden\n",
    "CDU = followed_accounts['CDU/CSU']\n",
    "SPD = followed_accounts['SPD']\n",
    "FDP = followed_accounts['FDP']\n",
    "LINKE = followed_accounts['Linke']\n",
    "GRUENE = followed_accounts['Gr√ºne']\n",
    "AFD = followed_accounts['AfD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"path\" gibt an, wo die Twitterdaten gespeichert sind\n",
    "path = 'recorded-tweets/chunk1'\n",
    "# in \"all_files\" werden alle Verzeichnisse der einzelnen Chunks gespeichert\n",
    "all_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.json')]\n",
    "\n",
    "# nun wird durch diese Verzeichnisse iteriert und die Chunks werden in eine einzelne Datei aneinander geh√§ngt\n",
    "# da es zu viele Dateien sind um sie alle auf einmal zu laden ohne unsere Laptops zu √ºberfordern, haben wir sie in einzelnen\n",
    "# Teilen geladen und weiter verarbeitet\n",
    "data1 = []\n",
    "for i in range(100):\n",
    "    file = pd.read_json(all_files[i], orient='index')\n",
    "    data1.append(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weiterverarbeitung der Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F√ºr unser Model wollen wir nur die Tweets benutzen, die tats√§chlich von einem Mitglied der jeweiligen Partei gepostet wurden.\n",
    "# Daher sortieren wir alle anderen Tweets aus und speichern nur die f√ºr uns relevanten \n",
    "tweets_afd1 = []\n",
    "\n",
    "for i in range(len(data1)):\n",
    "    tweets_afd1.append(get_tweets_by(AFD, data1[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aus der Liste mit allen Tweets wird nun ein String gemacht\n",
    "tweets_afd1_str = ''.join(map(str,tweets_afd1))\n",
    "# Der String wird mit der oben definierten Hilfsfunktion von ungewollten Symbolen befreit\n",
    "clean_afd1 = clean_text(tweets_afd1_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abspeichern der fertigen Textdatei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"tweets_afd1.txt\",\"w+\",encoding=\"utf8\", errors='ignore')     \n",
    "f.write(clean_afd1)\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
