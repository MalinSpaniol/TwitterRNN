{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter RNN - Bundestagswahlen in Deutschland 2017 \n",
    "## Maren, Sophia & Malin \n",
    "\n",
    "## Preprocessing \n",
    "### \"Chunk 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktionen die wir w√§hrend des Preprocessings anwenden \n",
    "\n",
    "''' Get all tweets by members of a certain party\n",
    "    input:  Party = list of the screennames of the members of a political party\n",
    "            all_tweets = a file with all tweets\n",
    "    output: all tweets by either of the screennames provided\n",
    "'''\n",
    "def get_tweets_by(party, all_tweets):\n",
    "    tweets = []\n",
    "    # go through all tweets\n",
    "    for i in range(len(all_tweets)):\n",
    "        # go through all members of the party\n",
    "        for member in party:\n",
    "            # if the screenname of the current tweet is in the list -> append\n",
    "            if (all_tweets[i]['user']['screen_name'] == member):\n",
    "                tweets.append(all_tweets[i]['text'])\n",
    "    return tweets\n",
    "\n",
    "''' Remove unwanted symbols from the text\n",
    "    input:  a string file\n",
    "    output: the string without the unwanted symbols\n",
    "'''\n",
    "def clean_text(text):\n",
    "    # remove URl links\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # remove @screenames\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)\n",
    "    # remove the indicator for a retweet \"RT\"\n",
    "    text = re.sub(r\"RT\", \"\", text)\n",
    "    #text = re.sub(r\"\\'\", \"\", text)\n",
    "    # remove emojis etc\n",
    "    text = re.sub(r\"üá©üá™\", \"\", text)\n",
    "    text = re.sub(r\"üòÅ\", \"\", text)\n",
    "    text = re.sub(r\"üòÄ\", \"\", text)\n",
    "    text = re.sub(r\"'‚ù§'\", \"\", text)\n",
    "    text = re.sub(r\"‚ù§\", \"\", text)\n",
    "    text = re.sub(r\"üéâ\", \"\", text)\n",
    "    text = re.sub(r\"'Ô∏è'\", \"\", text)\n",
    "    text = re.sub(r\"'\\S+\", \"\", text)\n",
    "   \n",
    "    # replace \"Umlaute\" and √ü\n",
    "    text = re.sub(r\"√º\", \"ue\", text)\n",
    "    text = re.sub(r\"√∂\", \"oe\", text)\n",
    "    text = re.sub(r\"√§\", \"ae\", text)\n",
    "    text = re.sub(r\"√ü\", \"ss\", text)\n",
    "    # add a space before each dot to keep it as its own token\n",
    "    text = re.sub(r\"\\.\", \" .\", text)\n",
    "    # remove single symbols\n",
    "    ''.join( c for c in text if  c not in '[,],/,:,&,_,1,2,3,4,5,6,7,8,9,0,,' )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √∂ffnen der json files, in denen gespeichert ist welche Account namen zu welcher Partei geh√∂ren\n",
    "with open('followed-accounts.json') as json_file:\n",
    "    followed_accounts = json.load(json_file)\n",
    "    \n",
    "CDU = followed_accounts['CDU/CSU']\n",
    "SPD = followed_accounts['SPD']\n",
    "FDP = followed_accounts['FDP']\n",
    "LINKE = followed_accounts['Linke']\n",
    "GRUENE = followed_accounts['Gr√ºne']\n",
    "AFD = followed_accounts['AfD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path wo die twitter daten gespeichert werden\n",
    "# wir √∂ffnen hier den zweiten von 3 chunks in die wir die 10GB Daten unterteilt haben \n",
    "# (um die Verarbeitung zu beschleunigen)\n",
    "\n",
    "path = 'recorded-tweets/chunk2'\n",
    "\n",
    "# geh in alle directories, wo die twitter daten gespeichert sind \n",
    "#wir haben die daten in 2 blocks unterteilt, damit das durchf√ºhren schneller geht\n",
    "\n",
    "all_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.json')]\n",
    "\n",
    "data1 = []\n",
    "for i in range(200):\n",
    "    file = pd.read_json(all_files[i], orient='index')\n",
    "    data1.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path wo die Twitterdaten liegen\n",
    "path = 'recorded-tweets/chunk2'\n",
    "# geh durch alle directories der twitterdaten durch\n",
    "all_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.json')]\n",
    "\n",
    "data2 = []\n",
    "for i in range(201,436):\n",
    "    file = pd.read_json(all_files[i], orient='index')\n",
    "    data2.append(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speichern der Tweets f√ºr jede Partei separat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AFD data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lege eine leere Liste um die Tweets einer Partei abzuspeichern. \n",
    "# Hier sind immer mal wieder Fehlermeldungen f√ºr einzelne Tweets gekommen, diese haben wir dann einfach manuell \n",
    "# √ºbersprungen, daher gibt es viele kleinere for Schleifen, anhand von einer Schleife, die durch alle durch l√§uft\n",
    "\n",
    "#hier wird die data1 Liste durchgelaufen\n",
    "\n",
    "tweets_AFD = []\n",
    "for i in range(76):\n",
    "    tweets_AFD.append(get_tweets_by(AFD, data1[i]))\n",
    "    \n",
    "for i in range(77,87):\n",
    "    tweets_AFD.append(get_tweets_by(AFD, data1[i]))\n",
    "\n",
    "for i in range(88,125):\n",
    "    tweets_AFD.append(get_tweets_by(AFD, data1[i]))\n",
    "    \n",
    "for i in range(126,172):\n",
    "    tweets_AFD.append(get_tweets_by(AFD, data1[i]))\n",
    "    \n",
    "for i in range(173,178):\n",
    "    tweets_AFD.append(get_tweets_by(AFD, data1[i]))\n",
    "    \n",
    "for i in range(179,200):\n",
    "    tweets_AFD.append(get_tweets_by(AFD, data1[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AFD data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier wird die data2 Liste durchgelaufen\n",
    "# angeh√§ngt werden die tweets jedoch in die gleiche Datei (tweets_AFD)\n",
    "for i in range(20):\n",
    "    tweets_AFD.append(get_tweets_by(AFD, data2[i]))\n",
    "    \n",
    "for i in range(21,76):\n",
    "    tweets_AFD.append(get_tweets_by(AFD, data2[i]))\n",
    "    \n",
    "for i in range(77,95):\n",
    "    tweets_AFD.append(get_tweets_by(AFD, data2[i]))    \n",
    "    \n",
    "for i in range(96, 122):\n",
    "    tweets_AFD.append(get_tweets_by(AFD, data2[i]))\n",
    "    \n",
    "for i in range(123,177):\n",
    "    tweets_AFD.append(get_tweets_by(AFD, data2[i]))\n",
    "    \n",
    "for i in range(178,235):\n",
    "    tweets_AFD.append(get_tweets_by(AFD, data2[i]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linke data 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# der gleiche Prozess wird nun f√ºr die anderen 5 Parteien durchlaufen\n",
    "# die Fehlermeldungen sind Partei unabh√§nging und scheinen am Tweet selbst zu liegen, daher ist die\n",
    "# Aufteilung der for Schleifen f√ºr jede Partei identisch\n",
    "\n",
    "tweets_LINKE = []\n",
    "for i in range(76):\n",
    "    tweets_LINKE.append(get_tweets_by(LINKE, data1[i]))\n",
    "    \n",
    "for i in range(77,87):\n",
    "    tweets_LINKE.append(get_tweets_by(LINKE, data1[i]))\n",
    "\n",
    "for i in range(88,125):\n",
    "    tweets_LINKE.append(get_tweets_by(LINKE, data1[i]))\n",
    "    \n",
    "for i in range(126,172):\n",
    "    tweets_LINKE.append(get_tweets_by(LINKE, data1[i]))\n",
    "    \n",
    "for i in range(173,178):\n",
    "    tweets_LINKE.append(get_tweets_by(LINKE, data1[i]))\n",
    "    \n",
    "for i in range(179,200):\n",
    "    tweets_LINKE.append(get_tweets_by(LINKE, data1[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linke data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    tweets_LINKE.append(get_tweets_by(LINKE, data2[i]))\n",
    "    \n",
    "for i in range(21,76):\n",
    "    tweets_LINKE.append(get_tweets_by(LINKE, data2[i]))\n",
    "    \n",
    "for i in range(77,95):\n",
    "    tweets_LINKE.append(get_tweets_by(LINKE, data2[i]))    \n",
    "    \n",
    "for i in range(96, 122):\n",
    "    tweets_LINKE.append(get_tweets_by(LINKE, data2[i]))\n",
    "    \n",
    "for i in range(123,177):\n",
    "    tweets_LINKE.append(get_tweets_by(LINKE, data2[i]))\n",
    "    \n",
    "for i in range(178,235):\n",
    "    tweets_LINKE.append(get_tweets_by(LINKE, data2[i]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDU data 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_CDU = []\n",
    "for i in range(76):\n",
    "    tweets_CDU.append(get_tweets_by(CDU, data1[i]))\n",
    "    \n",
    "for i in range(77,87):\n",
    "    tweets_CDU.append(get_tweets_by(CDU, data1[i]))\n",
    "\n",
    "for i in range(88,125):\n",
    "    tweets_CDU.append(get_tweets_by(CDU, data1[i]))\n",
    "    \n",
    "for i in range(126,172):\n",
    "    tweets_CDU.append(get_tweets_by(CDU, data1[i]))\n",
    "    \n",
    "for i in range(173,178):\n",
    "    tweets_CDU.append(get_tweets_by(CDU, data1[i]))\n",
    "    \n",
    "for i in range(179,200):\n",
    "    tweets_CDU.append(get_tweets_by(CDU, data1[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDU data 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    tweets_CDU.append(get_tweets_by(CDU, data2[i]))\n",
    "    \n",
    "for i in range(21,76):\n",
    "    tweets_CDU.append(get_tweets_by(CDU, data2[i]))\n",
    "    \n",
    "for i in range(77,95):\n",
    "    tweets_CDU.append(get_tweets_by(CDU, data2[i]))    \n",
    "    \n",
    "for i in range(96, 122):\n",
    "    tweets_CDU.append(get_tweets_by(CDU, data2[i]))\n",
    "    \n",
    "for i in range(123,177):\n",
    "    tweets_CDU.append(get_tweets_by(CDU, data2[i]))\n",
    "    \n",
    "for i in range(178,235):\n",
    "    tweets_CDU.append(get_tweets_by(CDU, data2[i]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1\n",
    "tweets_SPD = []\n",
    "for i in range(76):\n",
    "    tweets_SPD.append(get_tweets_by(SPD, data1[i]))\n",
    "    \n",
    "for i in range(77,87):\n",
    "    tweets_SPD.append(get_tweets_by(SPD, data1[i]))\n",
    "\n",
    "for i in range(88,125):\n",
    "    tweets_SPD.append(get_tweets_by(SPD, data1[i]))\n",
    "    \n",
    "for i in range(126,172):\n",
    "    tweets_SPD.append(get_tweets_by(SPD, data1[i]))\n",
    "    \n",
    "for i in range(173,178):\n",
    "    tweets_SPD.append(get_tweets_by(SPD, data1[i]))\n",
    "    \n",
    "for i in range(179,200):\n",
    "    tweets_SPD.append(get_tweets_by(SPD, data1[i]))\n",
    "    \n",
    "# data2\n",
    "\n",
    "for i in range(20):\n",
    "    tweets_SPD.append(get_tweets_by(SPD, data2[i]))\n",
    "    \n",
    "for i in range(21,76):\n",
    "    tweets_SPD.append(get_tweets_by(SPD, data2[i]))\n",
    "    \n",
    "for i in range(77,95):\n",
    "    tweets_SPD.append(get_tweets_by(SPD, data2[i]))    \n",
    "    \n",
    "for i in range(96, 122):\n",
    "    tweets_SPD.append(get_tweets_by(SPD, data2[i]))\n",
    "    \n",
    "for i in range(123,177):\n",
    "    tweets_SPD.append(get_tweets_by(SPD, data2[i]))\n",
    "    \n",
    "for i in range(178,235):\n",
    "    tweets_SPD.append(get_tweets_by(SPD, data2[i]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gr√ºne data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1\n",
    "tweets_GRUENE = []\n",
    "for i in range(76):\n",
    "    tweets_GRUENE.append(get_tweets_by(GRUENE, data1[i]))\n",
    "    \n",
    "for i in range(77,87):\n",
    "    tweets_GRUENE.append(get_tweets_by(GRUENE, data1[i]))\n",
    "\n",
    "for i in range(88,125):\n",
    "    tweets_GRUENE.append(get_tweets_by(GRUENE, data1[i]))\n",
    "    \n",
    "for i in range(126,172):\n",
    "    tweets_GRUENE.append(get_tweets_by(GRUENE, data1[i]))\n",
    "    \n",
    "for i in range(173,178):\n",
    "    tweets_GRUENE.append(get_tweets_by(GRUENE, data1[i]))\n",
    "    \n",
    "for i in range(179,200):\n",
    "    tweets_GRUENE.append(get_tweets_by(GRUENE, data1[i]))\n",
    "    \n",
    "    \n",
    "# data2\n",
    "for i in range(20):\n",
    "    tweets_GRUENE.append(get_tweets_by(GRUENE, data2[i]))\n",
    "    \n",
    "for i in range(21,76):\n",
    "    tweets_GRUENE.append(get_tweets_by(GRUENE, data2[i]))\n",
    "    \n",
    "for i in range(77,95):\n",
    "    tweets_GRUENE.append(get_tweets_by(GRUENE, data2[i]))    \n",
    "    \n",
    "for i in range(96, 122):\n",
    "    tweets_GRUENE.append(get_tweets_by(GRUENE, data2[i]))\n",
    "    \n",
    "for i in range(123,177):\n",
    "    tweets_GRUENE.append(get_tweets_by(GRUENE, data2[i]))\n",
    "    \n",
    "for i in range(178,235):\n",
    "    tweets_GRUENE.append(get_tweets_by(GRUENE, data2[i]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FDP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data1\n",
    "tweets_FDP = []\n",
    "for i in range(76):\n",
    "    tweets_FDP.append(get_tweets_by(FDP, data1[i]))\n",
    "    \n",
    "for i in range(77,87):\n",
    "    tweets_FDP.append(get_tweets_by(FDP, data1[i]))\n",
    "\n",
    "for i in range(88,125):\n",
    "    tweets_FDP.append(get_tweets_by(FDP, data1[i]))\n",
    "    \n",
    "for i in range(126,172):\n",
    "    tweets_FDP.append(get_tweets_by(FDP, data1[i]))\n",
    "    \n",
    "for i in range(173,178):\n",
    "    tweets_FDP.append(get_tweets_by(FDP, data1[i]))\n",
    "    \n",
    "for i in range(179,200):\n",
    "    tweets_FDP.append(get_tweets_by(FDP, data1[i]))\n",
    "    \n",
    "\n",
    "#data2\n",
    "for i in range(20):\n",
    "    tweets_FDP.append(get_tweets_by(FDP, data2[i]))\n",
    "    \n",
    "for i in range(21,76):\n",
    "    tweets_FDP.append(get_tweets_by(FDP, data2[i]))\n",
    "    \n",
    "for i in range(77,95):\n",
    "    tweets_FDP.append(get_tweets_by(FDP, data2[i]))    \n",
    "    \n",
    "for i in range(96, 122):\n",
    "    tweets_FDP.append(get_tweets_by(FDP, data2[i]))\n",
    "    \n",
    "for i in range(123,177):\n",
    "    tweets_FDP.append(get_tweets_by(FDP, data2[i]))\n",
    "    \n",
    "for i in range(178,235):\n",
    "    tweets_FDP.append(get_tweets_by(FDP, data2[i]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanen der strings, separat f√ºr jede Partei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier werden strings f√ºr jede Partei erstellt, und die clean Funktion wird angewendet\n",
    "str_AFD = ''.join(map(str,tweets_AFD))\n",
    "str_LINKE = ''.join(map(str,tweets_LINKE))\n",
    "str_CDU = ''.join(map(str,tweets_CDU))\n",
    "str_SPD = ''.join(map(str,tweets_SPD))\n",
    "str_GRUENE = ''.join(map(str,tweets_GRUENE))\n",
    "str_FDP = ''.join(map(str,tweets_FDP))\n",
    "\n",
    "clean_str_AFD = clean_text(str_AFD)\n",
    "clean_str_LINKE = clean_text(str_LINKE)\n",
    "clean_str_CDU = clean_text(str_CDU)\n",
    "clean_str_SPD = clean_text(str_SPD)\n",
    "clean_str_GRUENE = clean_text(str_GRUENE)\n",
    "clean_str_FDP = clean_text(str_FDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstelle .txt files - eine pro Partei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"AFD_2.txt\",\"w+\",encoding=\"utf8\", errors='ignore')     \n",
    "f.write(clean_str_AFD)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"LINKE_2.txt\",\"w+\",encoding=\"utf8\", errors='ignore')     \n",
    "f.write(clean_str_LINKE)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"CDU_2.txt\",\"w+\",encoding=\"utf8\", errors='ignore')     \n",
    "f.write(clean_str_CDU)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"SPD_2.txt\",\"w+\",encoding=\"utf8\", errors='ignore')     \n",
    "f.write(clean_str_SPD)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"GRUENE_2.txt\",\"w+\",encoding=\"utf8\", errors='ignore')     \n",
    "f.write(clean_str_GRUENE)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"FDP_2.txt\",\"w+\",encoding=\"utf8\", errors='ignore')     \n",
    "f.write(clean_str_FDP)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
