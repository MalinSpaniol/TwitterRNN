{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model\n",
    "## Maren, Sophia & Malin - Group 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dies ist das etwas komplexere Model, das wir eigentlich benutzen wollten um die neuen Tweets zu generieren. Es besteht aus einer \"Embedding Layer\" mit 50 Neuronen, zwei \"LSTM Layer\" mit je 100 Neuronen und zwei \"Fully-Connected Layer\", die erste mit 100 Neuronen und die zweite mit so vielen Neuronen wie es W√∂rter in unserem Vokabular gibt. Bei der Struktur des Models haben wir uns an einem Tutorial orientiert, das ein Word-Level Model in Keras zeigt (https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/). Weitere Erl√§uterungen und Problembeschreibungen sind als Kommentare im Code eingef√ºgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiere die ben√∂tigten packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import scipy.io\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "# zus√§tzliche Pakete f√ºr eine weitere Methode zu tokenisieren\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Diese Funktion entfernt alle Zeichen, die wir nicht haben wollen aus der Textdatei. Da die Tweets zun√§chst als Listenelemente\n",
    "    aneinander gereit wurden (siehe Preprocessing), sind in der Textdatei noch sehr viele eckige Klammern. Au√üerdem wurden \n",
    "    Zeilenumbr√ºche als '/n' codiert und Tweets, die die Standardl√§nge von 140 Zeichen √ºberschritten haben, wurden abgeschnitten,\n",
    "    sodass teilweise unvollst√§ndige Worte vorkommen. Zudem gibt es noch einige weitere ungew√ºnschte Zeichen, wie URL-links etc.\n",
    "    Input:  Eine String Datei\n",
    "    Output: Die Datei ohne die unerw√ºnschten Zeichen\n",
    "'''\n",
    "def clean_text(text):\n",
    "    \n",
    "    # entfernt URL Links\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # entfernt @screen_name\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)\n",
    "    # entfernt \"RT\" f√ºr Re-Tweet\n",
    "    text = re.sub(r\"RT\", \"\", text)\n",
    "    # entfernt Sondersymbole wie Emojis\n",
    "    text = re.sub(r\"üá©üá™\", \"\", text)\n",
    "    #text = re.sub(r\"üòÅ\", \"\", text)\n",
    "    #text = re.sub(r\"üòÄ\", \"\", text)\n",
    "    #text = re.sub(r\"'‚ù§'\", \"\", text)\n",
    "    #text = re.sub(r\"‚ù§\", \"\", text)\n",
    "    #text = re.sub(r\"üéâ\", \"\", text)\n",
    "    # entfernt Sonderzeichen\n",
    "    text = re.sub(r\"'Ô∏è\", \"\", text)\n",
    "    text = re.sub(r\"\\\\n\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\n\", \"\", text)\n",
    "    text = re.sub(r\"]\\S+\", \"\", text)\n",
    "    text = re.sub(r\"‚Ä¶\", \"\", text)\n",
    "    text = re.sub(r\". . . \", \"\", text)\n",
    "    text = re.sub(r\"r'\\S+\", \"\", text)\n",
    "    #text = re.sub(r\"#\", \"# \", text)\n",
    "    text = re.sub(r\"&amp\", \"\", text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"‚Äù\", \"\", text)\n",
    "    text = re.sub(r\"‚Äô\", \"\", text)\n",
    "    text = re.sub(r\"‚Ä¢\", \"\", text)\n",
    "    #text = re.sub(r\"+\", \"\", text)\n",
    "    text = re.sub(r\"¬ª\", \"\", text)\n",
    "    text = re.sub(r\"|\", \"\", text)\n",
    "    text = re.sub(r\"¬´\", \"\", text)\n",
    "    text = re.sub(r\"%\", \"\", text)\n",
    "    text = ''.join( c for c in text if  c not in '],[,/,:,&,_,1,2,3,4,5,6,7,8,9,0,\\,&,;,-,),(,?,!')\n",
    "   \n",
    "    # wir m√∂chten alle Buchstaben klein machen, um das Vokabular zu verringern\n",
    "    text = text.lower()\n",
    "    \n",
    "    # ersetzt Umlaute und √ü\n",
    "    text = re.sub(r\"√º\", \"ue\", text)\n",
    "    text = re.sub(r\"√∂\", \"oe\", text)\n",
    "    text = re.sub(r\"√§\", \"ae\", text)\n",
    "    text = re.sub(r\"√ü\", \"ss\", text)\n",
    "    \n",
    "    # setzt ein Leerzeichen vor jeden Punkt, sodass er als einzelnes Symbol gesehen wird und W√∂rter mit Punkt\n",
    "    # nicht als eigenst√§ndige W√∂rter ins Vokabular eingehen\n",
    "    text = re.sub(r\"\\.\", \" .\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einlesen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W√§hle ein Partei aus und kommentiere die restlichen Einlesefuntionen aus!\n",
    "\n",
    "# Lade die Textdatei ins Notebook. Der Text wird utf-8 encoded.\n",
    "# Zeichen, die von dieser Codierung nicht verstanden werden, werden erst einmal ignoriert.\n",
    "\n",
    "# with open(\"tweets_all_parties/GRUENE.txt\", encoding=\"utf8\", errors='ignore') as f:\n",
    "#     text = f.read()\n",
    "# input_text = clean_text(text)\n",
    "\n",
    "# with open(\"tweets_all_parties/LINKE.txt\", encoding=\"utf8\", errors='ignore') as f:\n",
    "#     text = f.read()  \n",
    "# input_text = clean_text(text)\n",
    "\n",
    "with open(\"tweets_all_parties/AFD.txt\", encoding=\"utf8\", errors='ignore') as f:\n",
    "    text = f.read()  \n",
    "input_text = clean_text(text)\n",
    "\n",
    "# with open(\"tweets_all_parties/FDP.txt\", encoding=\"utf8\", errors='ignore') as f:\n",
    "#     text = f.read()  \n",
    "# input_text = clean_text(text)\n",
    "\n",
    "# with open(\"tweets_all_parties/SPD.txt\", encoding=\"utf8\", errors='ignore') as f:\n",
    "#     text = f.read()  \n",
    "# input_text = clean_text(text)\n",
    "\n",
    "# with open(\"tweets_all_parties/CDU.txt\", encoding=\"utf8\", errors='ignore') as f:\n",
    "#     text = f.read()  \n",
    "# input_text = clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 1795\n",
      "text length: 5763\n"
     ]
    }
   ],
   "source": [
    "# Wir haben verschiede Versionen ausprobiert um den Text zu tokenisieren\n",
    "# Sie haben alle ihre Vor- und Nachteile, manche beschleunigen das Training, aber daf√ºr streichen sie alle Sonderzeichen,\n",
    "# andere wiederum sind langsamer, weil alle Sonderzeichen als einzelne \"W√∂rter\" interpretiert werden etc\n",
    "# Wir haben uns f√ºr die 4. Version entschieden, da sie ein guter Mittelweg ist das Vokabular klein zu halten und trotzdem\n",
    "# Smileys etc zu lernen.\n",
    "\n",
    "# 1. M√∂glichkeit - hier bleiben keine Punkte drin\n",
    "#tokenized_text = list(input_text.split(\" \"))\n",
    "#vocab = set(tokenized_text)\n",
    "\n",
    "# 2. M√∂glichkeit - einzelne Buchstaben und Satzzeichen sind weg\n",
    "#tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#tokenized_text = list(tokenizer.tokenize(text_char))\n",
    "#vocab = set(tokenized_text)\n",
    "\n",
    "# 3. M√∂glichkeit - hier werden alle Leerzeichen, Satzzeichen ets als einzelne \"W√∂rter\" interpretiert\n",
    "# dadurch ist dieser Ansatz sehr langsam\n",
    "#p2 = re.compile(r'(\\W+)')\n",
    "#tokenized_text = p2.split(input_text)\n",
    "\n",
    "# 4. M√∂glichkeit - beh√§lt smileys, Leerzeichen werden manuell am Ende wieder eingef√ºgt\n",
    "vocab = TreebankWordTokenizer()\n",
    "tokenized_text = vocab.tokenize(input_text)\n",
    "\n",
    "vocab = set(tokenized_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"vocabulary size: {}\".format(vocab_size))\n",
    "\n",
    "text_length = len(tokenized_text)\n",
    "print(\"text length: {}\".format(text_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kreiere Dictionaries um den Text in Zahlen umwandeln zu k√∂nnen \n",
    "# und sp√§ter die Zahlen wieder in Text\n",
    "# Gehe einmal durch ganze Vokabular und \"z√§hle mit\", sodass jedem Wort eine Zahl zugeordnet wird\n",
    "word_to_id = {word:i for i, word in enumerate(vocab)}\n",
    "id_to_word = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "# √úbersetze den Text zu den zugeh√∂rigen IDs\n",
    "text_idx = [word_to_id[word] for word in tokenized_text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generieren des Datensets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 5742\n"
     ]
    }
   ],
   "source": [
    "# Bestimme die L√§nge der einzelnen Sequenzen.\n",
    "# Wir haben uns erstmal f√ºr eine L√§nge von 10 W√∂rtern entschieden, da eine l√§ngere Abh√§ngigkeit von W√∂rtern in dem Kontext von\n",
    "# Tweets unserer Meinung nach wenig Sinn macht\n",
    "# Zu jeder Sequenz geh√∂rt ein Targetwort, welches das n√§chste Wort in der Reihe ist. Daher addiert man zu der L√§nge noch 1\n",
    "seq_len = 20+1\n",
    "# Erstelle eine leere Liste zum abspeichern der Sequenzen\n",
    "sequences = []\n",
    "# Starte vorne in der Liste und speicher die ersten 11 IDs als erste Sequenz, gehe zur zweiten ID usw.\n",
    "# Die letzte Sequenz endet mit dem letzten Wort als Target, dh das Wort an Stelle text_length - seq_len ist der Beginn der letzten Sequenz\n",
    "for i in range(seq_len, len(text_idx)):\n",
    "    # w√§hle die Sequenz von IDs aus\n",
    "    seq = text_idx[i-seq_len:i]\n",
    "    # und speicher sie in der Liste ab\n",
    "    sequences.append(seq)\n",
    "print('Total Sequences: {}'.format(len(sequences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zerteile die Sequenzen in Inputs der L√§nge seq_len und Target der L√§nge 1\n",
    "# Dazu mache die Liste erstmal zu einem Array\n",
    "sequences = np.array(sequences)\n",
    "# Nun zerteile die einzelnen Sequenzen\n",
    "input_data, target = sequences[:,:-1], sequences[:,-1]\n",
    "\n",
    "# Speicher die seq_length nocheinal dynamisch als die L√§nge der einzelnen Input Sequenzen ab, damit sie sich automatisch √§ndert,\n",
    "# wenn der Input sich √§ndert\n",
    "seq_length = input_data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Das Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorbereitungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Cast_1/x:0\", shape=(5742, 20), dtype=int32)\n",
      "Tensor(\"one_hot:0\", shape=(5742, 1795), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Jetzt fangen wir an das Model zu bauen\n",
    "# Daf√ºr brauchen wir ersteinmal ein Tensor Datenset\n",
    "\n",
    "# Wie immer setzen wir zun√§chst den Grafen zur√ºck\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Der Targetvektor wird one-hot encoded, damit er sp√§ter mit dem Output der Softmax Funktion verglichen werden kann\n",
    "# Daf√ºr muss er zun√§chst zu dem richtigen Datentyp gecastet werden. Das ist hier kein Problem, weil die IDs alle ganze Zahlen sind\n",
    "# und damit leicht als Integer statt als Float gespeichert werden k√∂nnen\n",
    "target = tf.cast(target, dtype = tf.int32)\n",
    "target = tf.one_hot(target, depth=vocab_size)\n",
    "# Der Input wird ebenfalls als Integer gespeichert, aber nicht als one-hot\n",
    "input_data = tf.cast(input_data, dtype = tf.int32)\n",
    "print(input_data)\n",
    "print(target)\n",
    "\n",
    "# Kreiere das Tensorflow Datenset mit den Input Sequenzen und dem one-hot Target\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_data,target))\n",
    "# Shuffle die Daten\n",
    "dataset = dataset.shuffle(buffer_size=len(sequences), reshuffle_each_iteration=True)\n",
    "\n",
    "# W√§hle ein Batchsize und zerteile das Datenset in Batches\n",
    "batchsize = 128\n",
    "dataset = dataset.batch(batchsize, drop_remainder=True)\n",
    "\n",
    "# Initialisiere den Iterator wie gehabt\n",
    "iterator = tf.data.Iterator.from_structure(dataset.output_types,dataset.output_shapes)\n",
    "iterator_init_op = iterator.make_initializer(dataset)\n",
    "\n",
    "# Generiere den Input Batch und den zugeh√∂rigen Target Batch mit Hilfe von .get_next()\n",
    "next_batch = iterator.get_next()\n",
    "input_data = next_batch[0]\n",
    "target_data = next_batch[1]\n",
    "\n",
    "\n",
    "# Initialisiere die Placeholder f√ºr den state der zwei LSTM layer\n",
    "# Der initial_state soll zun√§chst aus Nullen bestehen und dann immer wieder an das Model zur√ºck gegeben werden\n",
    "# Dies passiert sp√§ter im sess.run() mit der Hilfe von feed.dict{}\n",
    "# W√§hle die Gr√∂√üe der LSTM layer\n",
    "lstm_size = 100\n",
    "state_placeholder1 = tf.placeholder(tf.float32, [2, batchsize, lstm_size])\n",
    "state_placeholder2 = tf.placeholder(tf.float32, [2, batchsize, lstm_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architektur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding/EmbedSequence/embedding_lookup/Identity:0\", shape=(128, 20, 100), dtype=float32)\n",
      "Tensor(\"LSTM1/rnn/transpose_1:0\", shape=(128, 20, 100), dtype=float32)\n",
      "Tensor(\"LSTM2/GatherV2:0\", shape=(128, 100), dtype=float32)\n",
      "Tensor(\"fully_connected1/fully_connected/Relu:0\", shape=(128, 100), dtype=float32)\n",
      "Tensor(\"fully_connected2/fully_connected/BiasAdd:0\", shape=(128, 1795), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Definiere das Model\n",
    "# Zun√§chst wollen wir eine Embedding Layer, die die Embedding Vektoren der Input W√∂rter zur√ºck gibt und die Embedding Matrix trainiert\n",
    "with tf.variable_scope(\"embedding\", reuse=tf.AUTO_REUSE):\n",
    "    embedding = tf.contrib.layers.embed_sequence(\n",
    "        ids = input_data,\n",
    "        vocab_size = vocab_size,\n",
    "        embed_dim = 100,\n",
    "        unique = False,\n",
    "        trainable = True)\n",
    "    print(embedding)\n",
    "\n",
    "# Danach folgen zwei LSTM Layer mit jeweils 100 Neuronen\n",
    "with tf.variable_scope(\"LSTM1\", reuse=tf.AUTO_REUSE):\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(\n",
    "        num_units = lstm_size,\n",
    "        forget_bias=1.0,\n",
    "        state_is_tuple=True,\n",
    "        activation='tanh')\n",
    "    # der initial_state ist ein Tupel aus dem hidden state und dem cell state, beide werden gemeinsam durch den state_placeholder\n",
    "    # eingelesen und dann zu einem LSTMStateTupel gemacht\n",
    "    init_state = tf.nn.rnn_cell.LSTMStateTuple(state_placeholder1[0], state_placeholder1[1])\n",
    "    outputs1, states1 = tf.nn.dynamic_rnn(cell = cell, inputs = embedding, initial_state = init_state)\n",
    "    # wir wollen uns den aktuellen state merken um ihn in den n√§chsten batch zu geben\n",
    "    remember1 = states1\n",
    "    print(outputs1)\n",
    "    \n",
    "# Die zweite LSTM Layer ist aufgebaut wie die erste\n",
    "with tf.variable_scope(\"LSTM2\", reuse=tf.AUTO_REUSE):\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(\n",
    "        num_units = lstm_size,\n",
    "        num_proj = None,\n",
    "        forget_bias=1.0,\n",
    "        state_is_tuple=True,\n",
    "        activation='tanh')\n",
    "    init_state = tf.nn.rnn_cell.LSTMStateTuple(state_placeholder2[0],state_placeholder2[1])\n",
    "    outputs2, states2 = tf.nn.dynamic_rnn(cell = cell, inputs = outputs1, initial_state = init_state)\n",
    "    # wieder merken wir uns den state \n",
    "    remember2 = states2\n",
    "    # nach den LSTM Layern wollen wir nun nicht mehr den Output f√ºr jedes Wort in der Sequenz weiter geben\n",
    "    # sondern nur den Output des letzten Wortes in jeder Sequenz, weil wir nur das als prediction f√ºr das n√§chste Wort ben√∂tigen\n",
    "    # und daraus den Fehler zu dem tats√§chlichen letzten Wort berechnen k√∂nnen\n",
    "    # um an den letzten Output zu kommen ver√§ndern wir zun√§chste die Reihenfolde der drei Dimensionen des Outputs\n",
    "    # von (batchsize, seq_length, lstm_size) zu (seq_length, batchsize, lstm_size) damit f√ºr tf gather √ºber die seq_length \n",
    "    # anwenden k√∂nnen\n",
    "    temp = tf.transpose(outputs2, [1,0,2])\n",
    "    last_output = tf.gather(temp, int(temp.get_shape()[0])-1)\n",
    "    print(last_output)\n",
    "    \n",
    "\n",
    "# Die erste fully-connected layer besteht aus 100 Neuronen und benutzt die ReLU als activation function    \n",
    "with tf.variable_scope(\"fully_connected1\", reuse=tf.AUTO_REUSE):\n",
    "    full = tf.contrib.layers.fully_connected(\n",
    "        last_output,\n",
    "        100,\n",
    "        activation_fn = tf.nn.relu)\n",
    "    print(full)\n",
    "\n",
    "# Die zweite fully-connected layer hat so viele Neuronen, wie es W√∂rter in dem aktuellen Vokabular gibt.\n",
    "# Die activation function ist nun die Softmax, sodass wir den Output des Models mit dem Target vergleichen k√∂nnen\n",
    "with tf.variable_scope(\"fully_connected2\", reuse=tf.AUTO_REUSE):\n",
    "    # die logits werden f√ºr die Berechnung des Fehlers benutzt\n",
    "    logits = tf.contrib.layers.fully_connected(\n",
    "        full,\n",
    "        vocab_size,\n",
    "        activation_fn = None)\n",
    "    # out wird f√ºr die prediction des n√§chsten Wortes benutzt\n",
    "    out = tf.nn.softmax(logits)\n",
    "    print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition von Training Loss und der Optimierungsstrategie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der loss wird durch die cross entropy bestimmt, in der der Output des Models mit dem tats√§chlichen Target verglichen wird\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=target_data, logits=logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zum Optimieren benutzen wir den Adams Optimizer und eine Lernrate von 0.001\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "training_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Nr.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 6.132317\n",
      "\n",
      "\n",
      " jetzterstrecht sharia man soll Ô∏è‚É£üîπüáßÔ∏èüáÆÔ∏èüá±Ô∏èüá©Ô∏èüá∫Ô∏èüá≥Ô∏èüá¨Ô∏è‚Ñπeckpfeiler stellen # . .‚û°der lesenswert buerger sagen allen freie darauf schicksalswahl afdwahlkampfauftakt dann verhalten kanzlercheck \n",
      "\n",
      "\n",
      "Epoch: 1, Loss: 5.926611\n",
      "\n",
      "\n",
      " lagerarbeiter # # frage stinkt # # folgt . dem ihren .üì¢ nrw eigene sonntag stimmun jetzterstrecht wurzeln mit seehofer \n",
      "\n",
      "\n",
      "Epoch: 2, Loss: 5.773694\n",
      "\n",
      "\n",
      " hat bestimmt traudichdeutschland # # stefan wir rayk doch otte defendeurope afd in . baeuerlichen wollte groko aber koran zukunft \n",
      "\n",
      "\n",
      "Epoch: 3, Loss: 6.053644\n",
      "\n",
      "\n",
      " ist von gham das afd herzliches willkommen btwdas sieht vorstandsmitglied traudichdeutschland die antiafd traudichdeutschlan retweete . afd traudichdeutschland mehr dass \n",
      "\n",
      "\n",
      "Epoch: 4, Loss: 5.888480\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # wir starten damit die Variablen zu initialisieren\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # der \"saver\" speichert das fertig trainierte Model ab, damit es sp√§ter an jedem beliebigen Rechner wieder eingelesen werden kann\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # bestimme die Anzahl der Epochen\n",
    "    for epoch in range(180):\n",
    "    \n",
    "        # initialisiere einen Step counter f√ºr den Saver\n",
    "        global_step = 0\n",
    "        # lade das Datenset in den Iterator\n",
    "        sess.run(iterator_init_op)\n",
    "        \n",
    "        # am Anfang jeder neuen Epoche sollen die Placeholder f√ºr den jeweiligen state der beiden LSTM Layer mit Nullen initialisiert werden\n",
    "        # in jedem weiteren Step innerhalb der Epoche werden die states dann immer aufgerufen und weiter gegeben\n",
    "        state1 = np.zeros([2, batchsize, lstm_size], dtype = np.float32)\n",
    "        state2 = np.zeros([2, batchsize, lstm_size], dtype = np.float32)\n",
    "\n",
    "        # gehe durch das Datenset bis es leer ist\n",
    "        while True:\n",
    "            try:\n",
    "\n",
    "                # Starte das Training und gebe den loss aus\n",
    "                # au√üerdem gebe die states der beiden LSTM Layer aus um sie f√ºr den n√§chsten Batch zu benutzen\n",
    "                loss_val, state1, state2, _ = sess.run([loss, remember1, remember2, training_step],\n",
    "                                                      feed_dict = {state_placeholder1:state1, state_placeholder2:state2})\n",
    "                \n",
    "                # Erh√∂he den Global Step Count um 1\n",
    "                global_step += 1\n",
    "\n",
    "            # Stoppt sobald der Iterator leer ist\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "        \n",
    "        # Hier wird das Model abgespeichert \n",
    "        saver.save(sess, \"./checkpointAFD_LSTM/model.ckpt\", global_step = global_step)\n",
    "                \n",
    "       # Nach jeder Epoche soll der loss ausgegeben werden um den Trainingsprozess zu √ºberwachen\n",
    "        print(\"Epoch: {}, Loss: {:f}\".format(epoch, loss_val))\n",
    "        \n",
    "        \n",
    "        # Wir wollen schon w√§hrend des Trainings Tweets generieren um zu schauen, wie sich die Qualit√§t ver√§ndert\n",
    "        \n",
    "        # Bestimme die L√§nge der generierten Tweets\n",
    "        tweet_length = 20\n",
    "        \n",
    "        # Als Startsequenz f√ºrs Sampling k√∂nnen wir nicht wie bisher einfach eine zuf√§llige Sequenz w√§hlen,\n",
    "        # da unser Model einen Input in Batches erwartet\n",
    "        # deshalb muss auch der Sampling Input ein Batch sein\n",
    "        # wir haben das so gel√∂st, dass wir einen Batch gebaut haben, der nur aus Sequenzen mit Nullen besteht und erst an \n",
    "        # der letzten Stelle die zuf√§llige Sequenz hat\n",
    "        \n",
    "        # dazu w√§hlen wir zun√§chst zuf√§llig eine Startsequenz aus\n",
    "        start = random.randint(0, len(text_idx) - seq_length)\n",
    "        # und machen sie zu einem Tensor\n",
    "        random_sequence = tf.constant(text_idx[start:start + seq_length], shape = (1,seq_length), dtype = tf.int32)\n",
    "        # au√üerdem brauchen wir noch batchsize-1 leere Sequenzen\n",
    "        seq_template = tf.zeros([batchsize-1,seq_length], dtype=tf.int32)\n",
    "        # die leeren und die eine zuf√§llige Sequenz packen wir dann zusammen f√ºr unseren Input\n",
    "        random_seq_batch = tf.concat([seq_template, random_sequence], axis = 0)\n",
    "        \n",
    "        # Erstelle eine leere Liste um die generierten W√∂rter abzuspeichern\n",
    "        sampled_words = []\n",
    "        \n",
    "        # Generiere Wort f√ºr Wort einen euen Tweet\n",
    "        for n in range(tweet_length):\n",
    "            \n",
    "            # Da wir die generierten Tweets nur anschauen wollen und nicht mit einem Target vergleichen, brauchen wir Faketargets\n",
    "            fake_target = tf.zeros([batchsize, vocab_size], dtype=tf.float32)\n",
    "            sample_dataset = tf.data.Dataset.from_tensor_slices(([random_seq_batch], [fake_target]))\n",
    "            # Lade das Sample Datenset in den Iterator\n",
    "            sess.run(iterator.make_initializer(sample_dataset))\n",
    "    \n",
    "            # Lese den Softmax Output der letzten Layer aus (dieser liefert die Wahrscheinlichkeiten f√ºr das n√§chste generierte Wort)\n",
    "            # und gebe den state der LSTM Layer immer wieder neu rein\n",
    "            sample_output, state1, state2 = sess.run([out, remember1, remember2],\n",
    "                                                     feed_dict={state_placeholder1: state1, state_placeholder2:state2})\n",
    "\n",
    "            # Wir wollen nur den letzten Output, da dieser von unserer random Startsequenz generiert wurde\n",
    "            last = sample_output[-1,:]\n",
    "            # Wir w√§hlen ein Wort \"zuf√§llig\" aber mit der Wahrscheinlichkeit p, die uns unser Output gibt,\n",
    "            # aus unserem Vokabular aus...\n",
    "            sample = np.random.choice(range(vocab_size), p=last.ravel())\n",
    "            # ...und h√§ngen es an die Liste der generierten W√∂rter an\n",
    "            sampled_words.append(sample)\n",
    "            \n",
    "            # Update die Startsequenz mit dem neuen Wort um das n√§chste Wort zu generieren\n",
    "            # Dies ist wieder etwas komplizierter, weil wir zun√§chst aus dem Batch die letzte Sequenz haben wollen\n",
    "            # also die, die nicht aus Nullen besteht\n",
    "            temp = random_seq_batch[batchsize-1,:]\n",
    "            # an diese h√§ngen wir dann das neue Wort dran und schneiden das erste ab\n",
    "            next_random_sequence = tf.concat([temp[1:], tf.constant(sample,shape = (1,), dtype = tf.int32)], axis = 0)\n",
    "            # dann muss sie noch in die richtige Form gebracht werden\n",
    "            next_random_sequence = tf.reshape(next_random_sequence, shape = [1,seq_length])\n",
    "            # bevor sie wieder mit den Nuller-Sequenzen zu einem Batch verbunden werden kann und als n√§chsten Input dient\n",
    "            random_sequence = tf.concat([seq_template, next_random_sequence], axis = 0)\n",
    "      \n",
    "        # Zeige die generierten Tweets an\n",
    "        # da wir in dieser Tokenisierung keine Leerzeichen haben, muss zwischen jedes Wort eins gesetzt werden\n",
    "        sample_txt = ' '.join(id_to_word[idx] for idx in sampled_words)\n",
    "        print('\\n\\n %s \\n\\n' % (sample_txt,))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
