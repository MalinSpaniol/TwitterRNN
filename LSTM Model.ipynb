{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dies ist das etwas komplexere Model, das wir eigentlich benutzen wollten um die neuen Tweets zu generieren. Es besteht aus einer \"Embedding Layer\" mit 50 Neuronen, zwei \"LSTM Layer\" mit je 100 Neuronen und zwei \"Fully-Connected Layer\", die erste mit 100 Neuronen und die zweite mit so vielen Neuronen wie es W√∂rter in unserem Vokabular gibt. Bei der Struktur des Models haben wir uns an einem Tutorial orientiert, das ein Word-Level Model in Keras zeigt (https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/). Weitere Erl√§uterungen und Problembeschreibungen sind als Kommentare im Code eingef√ºgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiere die ben√∂tigten packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import scipy.io\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lade die Textdatei ins Notebook. Der Text wird utf-8 encoded.\n",
    "# Zeichen, die von dieser Codierung nicht verstanden werden, werden erst einmal ignoriert.\n",
    "with open(\"tweets/LINKE.txt\", encoding=\"utf8\", errors='ignore') as f:\n",
    "    textLINKE = f.read()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Diese Funktion entfernt alle Zeichen, die wir nicht haben wollen aus der Textdatei. Da die Tweets zun√§chst als Listenelemente\n",
    "    aneinander gereit wurden (siehe Preprocessing), sind in der Textdatei noch sehr viele eckige Klammern. Au√üerdem wurden \n",
    "    Zeilenumbr√ºche als '/n' codiert und Tweets, die die Standardl√§nge von 140 Zeichen √ºberschritten haben, wurden abgeschnitten,\n",
    "    sodass teilweise unvollst√§ndige Worte vorkommen. Zudem gibt es noch einige weitere ungew√ºnschte Zeichen, wie URL-links etc.\n",
    "    Input:  Eine String Datei\n",
    "    Output: Die Datei ohne die unerw√ºnschten Zeichen\n",
    "'''\n",
    "def clean_text(text):\n",
    "    \n",
    "    # entfernt URL Links\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # entfernt @screen_name\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)\n",
    "    # entfernt \"RT\" f√ºr Re-Tweet\n",
    "    text = re.sub(r\"RT\", \"\", text)\n",
    "    # entfernt Sondersymbole wie Emojis\n",
    "    text = re.sub(r\"üá©üá™\", \"\", text)\n",
    "    #text = re.sub(r\"üòÅ\", \"\", text)\n",
    "    #text = re.sub(r\"üòÄ\", \"\", text)\n",
    "    #text = re.sub(r\"'‚ù§'\", \"\", text)\n",
    "    #text = re.sub(r\"‚ù§\", \"\", text)\n",
    "    #text = re.sub(r\"üéâ\", \"\", text)\n",
    "    # entfernt Sonderzeichen\n",
    "    text = re.sub(r\"'Ô∏è\", \"\", text)\n",
    "    text = re.sub(r\"‚Ä¶\", \"\", text)\n",
    "    text = re.sub(r\"\\\\n\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\n\", \"\", text)\n",
    "    text = re.sub(r\"]\\S+\", \"\", text)\n",
    "    # entfernt einzelne Symbole \n",
    "    text = ''.join( c for c in text if  c not in '[,],/,:,&,_,1,2,3,4,5,6,7,8,9,0,,' )\n",
    "\n",
    "    # ersetzt Umlaute und √ü\n",
    "    text = re.sub(r\"√º\", \"ue\", text)\n",
    "    text = re.sub(r\"√∂\", \"oe\", text)\n",
    "    text = re.sub(r\"√§\", \"ae\", text)\n",
    "    text = re.sub(r\"√ü\", \"ss\", text)\n",
    "    \n",
    "    # setzt ein Leerzeichen vor jeden Punkt, sodass er als einzelnes Symbol gesehen wird und W√∂rter mit Punkt\n",
    "    # nicht als eigenst√§ndige W√∂rter ins Vokabular eingehen\n",
    "    text = re.sub(r\"\\.\", \" .\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2596\n",
      "text lenght: 7379\n"
     ]
    }
   ],
   "source": [
    "# Benutze die clean_text Funktion um den Text von ungewollten Symbolen zu befreien\n",
    "text = clean_text(textLINKE)\n",
    "\n",
    "# Wir haben verschiede Versionen ausprobiert um den Text zu tokenisieren\n",
    "# Sie haben alle ihre Vor- und Nachteile, manche beschleunigen das Training, aber daf√ºr streichen sie alle Sonderzeichen,\n",
    "# andere wiederum sind langsamer, weil alle Sonderzeichen als einzelne \"W√∂rter\" interpretiert werden etc\n",
    "\n",
    "# 1. M√∂glichkeit\n",
    "tokenized_text = list(text.split(\" \"))\n",
    "vocab = set(tokenized_text)\n",
    "\n",
    "# 2. M√∂glichkeit - tokenisiert den Text, aber l√∂scht einzelne Buchstaben und Satzzeichen\n",
    "#tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#tokenized_text = list(tokenizer.tokenize(text_char))\n",
    "#vocab = set(tokenized_text)\n",
    "\n",
    "# Speicher die L√§nge des Vokabulars und des Textes als einzelne Variablen ab um sp√§ter darauf zugreifen zu k√∂nnen\n",
    "vocab_size = len(vocab)\n",
    "print(\"vocab size: {}\".format(vocab_size))\n",
    "\n",
    "text_len = len(tokenized_text)\n",
    "print(\"text lenght: {}\".format(text_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kreiere Dictionaries um den Text in Zahlen umwandeln zu k√∂nnen \n",
    "# und sp√§ter die Zahlen wieder in Text\n",
    "# Gehe einmal durch ganze Vokabular und \"z√§hle mit\", sodass jedem Wort eine Zahl zugeordnet wird\n",
    "word_to_id = {word:i for i, word in enumerate(vocab)}\n",
    "id_to_word = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "# √úbersetze den Text zu den zugeh√∂rigen IDs\n",
    "text_idx = [word_to_id[word] for word in tokenized_text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generieren des Datensets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 7358\n"
     ]
    }
   ],
   "source": [
    "# Bestimme die L√§nge der einzelnen Sequenzen.\n",
    "# Wir haben uns erstmal f√ºr eine L√§nge von 20 W√∂rtern entschieden, da eine l√§ngere Abh√§ngigkeit von W√∂rtern in dem Kontext von\n",
    "# Tweets unserer Meinung nach wenig Sinn macht\n",
    "# Zu jeder Sequenz geh√∂rt ein Targetwort, welches das n√§chste Wort in der Reihe ist. Daher addiert man zu der L√§nge noch 1\n",
    "seq_len = 20+1\n",
    "# Erstelle eine leere Liste zum abspeichern der Sequenzen\n",
    "sequences = []\n",
    "# Starte vorne in der Liste und speicher die ersten 21 IDs als erste Sequenz, gehe zur zweiten ID usw.\n",
    "# Die letzte Sequenz endet mit dem letzten Wort als Target, dh das Wort an Stelle text_len - seq_len ist der Beginn der letzten Sequenz\n",
    "for i in range(seq_len, len(text_idx)):\n",
    "    # w√§hle die Sequenz von IDs aus\n",
    "    seq = text_idx[i-seq_len:i]\n",
    "    # und speicher sie in der Liste ab\n",
    "    sequences.append(seq)\n",
    "print('Total Sequences: {}'.format(len(sequences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zerteile die Sequenzen in Inputs der L√§nge seq_len und Target der L√§nge 1\n",
    "# Dazu mache die Liste erstmal zu einem Array\n",
    "sequences = np.array(sequences)\n",
    "# Nun zerteile die einzelnen Sequenzen\n",
    "input_data, target = sequences[:,:-1], sequences[:,-1]\n",
    "\n",
    "# Speicher die seq_length nocheinal dynamisch als die L√§nge der einzelnen Input Sequenzen ab, damit sie sich automatisch √§ndert,\n",
    "# wenn der Input sich √§ndert\n",
    "seq_length = input_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jetzt fangen wir an das Model zu bauen\n",
    "# Daf√ºr brauchen wir ersteinmal ein Tensor Datenset\n",
    "\n",
    "# Wie immer resetten wir den default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Der Targetvektor wird one-hot encoded, damit er sp√§ter mit dem Output der Softmax Funktion verglichen werden kann\n",
    "# Daf√ºr muss er zun√§chst zu dem richtigen Datentyp gecastet werden. Das ist hier kein Problem, weil die IDs alle ganze Zahlen sind\n",
    "# und damit leicht als Integer statt als Float gespeichert werden k√∂nnen\n",
    "target = tf.cast(target, dtype = tf.int32)\n",
    "target = tf.one_hot(target, depth=vocab_size)\n",
    "# Der Input wird ebenfalls als Integer gespeichert, aber nicht als one-hot\n",
    "input_data = tf.cast(input_data, dtype = tf.int32)\n",
    "\n",
    "# Kreiere das Tensorflow Datenset mit den Input Sequenzen und dem one-hot Target\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_data,target))\n",
    "\n",
    "# W√§hle ein Batchsize und zerteile das Datenset in Batches\n",
    "batchsize = 128\n",
    "dataset = dataset.batch(batchsize)\n",
    "\n",
    "# Initialisiere den Iterator wie gehabt\n",
    "iterator = tf.data.Iterator.from_structure(dataset.output_types,dataset.output_shapes)\n",
    "iterator_init_op = iterator.make_initializer(dataset)\n",
    "\n",
    "# Generiere den Input Batch und den zugeh√∂rigen Target Batch mit Hilfe von .get_next()\n",
    "next_batch = iterator.get_next()\n",
    "input_data = next_batch[0]\n",
    "target_data = next_batch[1]\n",
    "\n",
    "# Initialisiere die Placeholder f√ºr den state der zwei LSTM layer\n",
    "# Der initial_state soll zun√§chst aus Nullen bestehen und dann immer wieder an das Model zur√ºck gegeben werden\n",
    "# Dies passiert sp√§ter im sess.run() mit der Hilfe von feed.dict{}\n",
    "# W√§hle die Gr√∂√üe der LSTM layer\n",
    "lstm_size = 100\n",
    "lstm_state1 = tf.placeholder(shape=[seq_length, lstm_size], dtype=np.float32)\n",
    "lstm_state2 = tf.placeholder(shape=[seq_length, lstm_size], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Das Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding/EmbedSequence/embedding_lookup/Identity:0\", shape=(?, 20, 50), dtype=float32)\n",
      "128\n",
      "Tensor(\"LSTM1/lstm_cell/mul_2:0\", shape=(20, 100), dtype=float32)\n",
      "LSTMStateTuple(c=<tf.Tensor 'LSTM1/lstm_cell/add_255:0' shape=(20, 100) dtype=float32>, h=<tf.Tensor 'LSTM1/lstm_cell/mul_383:0' shape=(20, 100) dtype=float32>)\n",
      "128\n",
      "Tensor(\"LSTM2/lstm_cell/mul_2:0\", shape=(20, 100), dtype=float32)\n",
      "LSTMStateTuple(c=<tf.Tensor 'LSTM2/lstm_cell/add_255:0' shape=(20, 100) dtype=float32>, h=<tf.Tensor 'LSTM2/lstm_cell/mul_383:0' shape=(20, 100) dtype=float32>)\n",
      "Tensor(\"fully_connected1/fully_connected/Relu:0\", shape=(128, 20, 100), dtype=float32)\n",
      "Tensor(\"fully_connected2/fully_connected/Softmax:0\", shape=(128, 20, 2596), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Definiere das Model\n",
    "# Zun√§chst wollen wir eine Embedding Layer, die die Embedding Vektoren der Input W√∂rter zur√ºck gibt und die EMbedding Matrix trainiert\n",
    "with tf.variable_scope(\"embedding\", reuse=tf.AUTO_REUSE):\n",
    "    embedding = tf.contrib.layers.embed_sequence(\n",
    "        ids = input_data,\n",
    "        vocab_size = vocab_size,\n",
    "        embed_dim = 50,\n",
    "        unique = False,\n",
    "        trainable = True)\n",
    "    print(embedding)\n",
    "\n",
    "# Danach folgen zwei LSTM Layer mit jeweils 100 Neuronen\n",
    "with tf.variable_scope(\"LSTM1\", reuse=tf.AUTO_REUSE):\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(\n",
    "        num_units = lstm_size,\n",
    "        forget_bias=1.0,\n",
    "        state_is_tuple=True,\n",
    "        activation='tanh')\n",
    "    # Der Initial_state wird zun√§chst auf Null gesetzt\n",
    "    # Hier soll eigentlich der Placeholder reingegeben werden, aber der einfachheitshalber haben wir es ersteinmal ohne probiert\n",
    "    initial_state = cell.zero_state(seq_length, dtype=tf.float32)\n",
    "    #initial_state = lstm_state1\n",
    "    \n",
    "    # Die Sequenzen im Input Batch werden einzelnen in die LSTM Zelle eingespeist und verarbeitet \n",
    "    lstm1 = []\n",
    "    for batch in range(batchsize):\n",
    "        lstm_outputs, final_state = cell(embedding[batch], initial_state)\n",
    "        lstm1.append(lstm_outputs)\n",
    "    # Der letzte final_state, in dem sich die Zelle befindet, wird abgespeichert und im n√§chsten Batch eingelesen\n",
    "    remember1 = final_state\n",
    "    print(len(lstm1))\n",
    "    print(lstm1[0])\n",
    "    print(remember1)\n",
    "    \n",
    "# Die zweite LSTM Layer ist aufegabut wie die erste\n",
    "with tf.variable_scope(\"LSTM2\", reuse=tf.AUTO_REUSE):\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(\n",
    "        num_units = lstm_size,\n",
    "        forget_bias=1.0,\n",
    "        state_is_tuple=True,\n",
    "        activation='tanh')\n",
    "    initial_state = cell.zero_state(seq_length, tf.float32)\n",
    "    #initial_state = lstm_state2\n",
    "    lstm2 = []\n",
    "    for batch in lstm1:\n",
    "        lstm_outputs, final_state = cell(batch, initial_state)\n",
    "        lstm2.append(lstm_outputs)\n",
    "    remember2 = final_state\n",
    "    print(len(lstm2))\n",
    "    print(lstm2[0])\n",
    "    print(remember2)\n",
    "    \n",
    "# Die erste fully-connected layer besteht aus 100 Neuronen und benutzt die ReLU als activation function    \n",
    "with tf.variable_scope(\"fully_connected1\", reuse=tf.AUTO_REUSE):\n",
    "    full = tf.contrib.layers.fully_connected(\n",
    "        lstm2,\n",
    "        100,\n",
    "        activation_fn = tf.nn.relu)\n",
    "    print(full)\n",
    "\n",
    "# Die zweite fully-connected layer hat so viele Neuronen, wie es W√∂rter in dem aktuellen Vokabular gibt.\n",
    "# Die activation function ist nun die Softmax, sodass wir den Output des Models mit dem Target vergleichen k√∂nnen\n",
    "with tf.variable_scope(\"fully_connected2\", reuse=tf.AUTO_REUSE):\n",
    "    logits = tf.contrib.layers.fully_connected(\n",
    "        full,\n",
    "        vocab_size,\n",
    "        activation_fn = tf.nn.softmax)\n",
    "    print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problembeschreibung\n",
    "\n",
    "Das Model l√§uft zwar bis zu diesem Zeitpunkt ohne Fehlermeldung, das Problem ist aber, dass die Output Dimensionen nicht stimmen. Bis zur zweiten LSTM Layer passt noch alles - die Embedding Layer gibt einen Ouput mit Dimensionen (batch_size, seq_length, embed_size) weiter und die erste LSTM Layer produziert eine Liste mit der L√§nge batch_size, in der jeder Eintrag die Dimensionen (seq_length, lstm_size) hat. Die zweite LSTM Layer prdoziert einen Output mit den gleichen Dimensionen, sollte aber eigentlich nur die Dimensionen (batch_size, lstm_size) haben. Diese falsche \"Extra-Dimension\" zieht sich dann nat√ºrlich auch durch die beiden fully-connectes Layer, sodass die Logits am Ende eine Gr√∂√üe von (seq_length * batch_size, vocab_size) statt nur (batch_size, vocab_size) haben. Vermutlich kommt der Fehler daher, dass die LSTM Layer eine Liste produziert statt eines Tensors. Aber auch nach viel rumexperimenieren und nachlesen konnten wir noch keine L√∂sung f√ºr das Problem finden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition von Training Loss und der Optimierungsstrategie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der loss wird durch die cross entropy bestimmt, in der der Output des Models mit dem tats√§chlichen Target verglichen wird\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=target_data, logits=logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zum Optimieren benutzen wir den Adams Optimizer und eine Lernrate von 0.001\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 1e-3)\n",
    "training_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problembeschreibung\n",
    "\n",
    "Hier kommt unser zweites Problem auf. Wir schaffen es immer noch nicht die Placeholder korrekt einzulesen.\n",
    "Deshalb haben wir zwei Sessions geschrieben. Diese ist so wie wie wir uns das im Idealfall vorgestellt haben. Der final_state in jeder LSTM Layer wird in der Variabel \"remember1\" bzw \"remember2\" gespeichert und ausgelesen und anschlie√üend wieder in den Placeholder eingespeist, sodass der n√§chste Batch den final_state des vorherigen Batches als initial_state hat.\n",
    "\n",
    "Die zweite eine Zelle weiter unten ist die, mit der wir zun√§chste gearbeitet haben um das Model zu bauen. Sie hat keine Placeholder ist aber ansonsten gleich. In der unteren kann man den Fehler sehen, den wir oben beschrieben haben. N√§mlich das die Logits die falsche Shape haben.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Nr.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-10957adb63f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[1;31m# au√üerdem gebe die states der beiden LSTM Layer aus um sie f√ºr den n√§chsten Batch zu benutzen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 loss_val, state1, state2, _ = sess.run([loss, remember1, remember2, training_step],\n\u001b[1;32m---> 27\u001b[1;33m                                                       feed_dict = {lstm_state1:state1, lstm_state2:state2})\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[1;31m# Erh√∂he den Global Step Count um 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # wir starten damit die Variablen zu initialisieren\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # bestimme die Anzahl der Epochen\n",
    "    for epoch in range(100):\n",
    "    \n",
    "        # initialisiere einen Step counter um an den richtigen Stellen die Placeholder einzuspeisen\n",
    "        global_step = 0\n",
    "        # lade das Datenset in den Iterator\n",
    "        sess.run(iterator_init_op)\n",
    "\n",
    "        # gehe durch das Datenset bis es leer ist\n",
    "        while True:\n",
    "            try:\n",
    "                # Falls dies der erste Step ist, initialisiere die initial_states der beiden LSTM Layer mit Nullen \n",
    "                if global_step == 0:\n",
    "                    state1 = np.zeros([seq_length,lstm_size], dtype = np.float32)\n",
    "                    state2 = np.zeros([seq_length,lstm_size], dtype = np.float32)\n",
    "\n",
    "                # Starte das Training und gebe den loss aus\n",
    "                # au√üerdem gebe die states der beiden LSTM Layer aus um sie f√ºr den n√§chsten Batch zu benutzen\n",
    "                loss_val, state1, state2, _ = sess.run([loss, remember1, remember2, training_step],\n",
    "                                                      feed_dict = {lstm_state1:state1, lstm_state2:state2})\n",
    "                \n",
    "                # Erh√∂he den Global Step Count um 1\n",
    "                global_step += 1\n",
    "\n",
    "            # Stoppt sobald der Iterator leer ist\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "                \n",
    "       # Nach jeder Epoche soll der loss ausgegeben werden um den Trainingsprozess zu √ºberwachen\n",
    "        print(\"Epoch: {}, Loss: {:f}\".format(epoch, loss_val))\n",
    "        \n",
    "        # Wir wollen schon w√§hrend des Trainings Tweets generieren um zu schauen, wie sich die Qualit√§t ver√§ndert\n",
    "        \n",
    "        # Bestimme die L√§nge der generierten Tweets\n",
    "        tweet_length = 20\n",
    "        \n",
    "        # W√§hle zuf√§llig eine Startsequenz aus\n",
    "        start_idx = random.randint(0, len(text_idx) - seq_len)\n",
    "        seq_idx = text_idx[start_idx:start_idx + seq_len]      \n",
    "      \n",
    "        # Erstelle eine leere Liste um die generierten W√∂rter abzuspeichern\n",
    "        sample_seq_idx = []\n",
    "        \n",
    "        # Generiere Wort f√ºr Wort einen euen Tweet\n",
    "        for n in range(tweet_length):\n",
    "            \n",
    "            # Da wir die generierten Tweets nur anschauen wollen und nicht mit einem Target vergleichen, brauchen wir Faketargets\n",
    "            fake_target = np.zeros([1,tweet_length], dtype=np.int32)\n",
    "            sample_dataset = tf.data.Dataset.from_tensor_slices(([seq_idx], fake_target))\n",
    "            # Lade das Sample Datenset in den Iterator\n",
    "            sess.run(iterator.make_initializer(sample_dataset))\n",
    "    \n",
    "            # Lese den Softmax Output der letzten Layer aus (dieser liefert die Wahrscheinlichkeiten f√ºr das n√§chste generierte Wort)\n",
    "            # und gebe den state der LSTM Layer immer wieder neu rein\n",
    "            sample_output, state1, state2 = sess.run([logits, remember1, remember2],\n",
    "                                                     feed_dict={lstm_state1: state1, lstm_state2:state2})\n",
    "\n",
    "            \n",
    "            # W√§hle zuf√§llig ein Wort aus der Softmax Distribution und h√§nge es an die Liste an\n",
    "            sample = np.random.choice(range(vocab_size), p=sample_output.ravel())\n",
    "            sample_seq_idx.append(sample)\n",
    "            # Update die Startsequenz mit dem neuen Wort um das n√§chste Wort zu generieren\n",
    "            seq_idx = seq_idx[1:] + [sample]\n",
    "        \n",
    "      \n",
    "        # Zeige den generierten Tweets an\n",
    "        # da wir in dieser Tokenisierung keine Leerzeichen haben, muss zwischen jedes Wort eins gesetzt werden\n",
    "        sample_txt = ' '.join(idx_to_char[idx] for idx in sample_seq_idx)\n",
    "        print('----\\n %s \\n----\\n' % (sample_txt,))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Nr.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits and labels must be broadcastable: logits_size=[2560,2596] labels_size=[128,2596]\n\t [[{{node softmax_cross_entropy_with_logits}} = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](softmax_cross_entropy_with_logits/Reshape, IteratorGetNext:1)]]\n\nCaused by op 'softmax_cross_entropy_with_logits', defined at:\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\asyncio\\base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\asyncio\\base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 370, in dispatch_queue\n    yield self.process_one()\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\gen.py\", line 1080, in __init__\n    self.run()\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2817, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2843, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3018, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3183, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3265, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-60-9779c50287bd>\", line 2, in <module>\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=target_data, logits=logits))\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1888, in softmax_cross_entropy_with_logits_v2\n    precise_logits, labels, name=name)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 7747, in softmax_cross_entropy_with_logits\n    name=name)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3272, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1768, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): logits and labels must be broadcastable: logits_size=[2560,2596] labels_size=[128,2596]\n\t [[{{node softmax_cross_entropy_with_logits}} = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](softmax_cross_entropy_with_logits/Reshape, IteratorGetNext:1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1292\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1293\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1277\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: logits and labels must be broadcastable: logits_size=[2560,2596] labels_size=[128,2596]\n\t [[{{node softmax_cross_entropy_with_logits}} = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](softmax_cross_entropy_with_logits/Reshape, IteratorGetNext:1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-2029f097a532>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[1;31m# Starte das Training und gebe den loss aus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[1;31m# au√üerdem gebe die states der beiden LSTM Layer aus um sie f√ºr den n√§chsten Batch zu benutzen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                 \u001b[0mloss_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[1;31m# Erh√∂he den Global Step Count um 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 887\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    888\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1110\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1286\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1287\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1306\u001b[0m           self._config.experimental.client_handles_error_formatting):\n\u001b[0;32m   1307\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1308\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1310\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: logits and labels must be broadcastable: logits_size=[2560,2596] labels_size=[128,2596]\n\t [[{{node softmax_cross_entropy_with_logits}} = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](softmax_cross_entropy_with_logits/Reshape, IteratorGetNext:1)]]\n\nCaused by op 'softmax_cross_entropy_with_logits', defined at:\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\asyncio\\base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\asyncio\\base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 370, in dispatch_queue\n    yield self.process_one()\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\gen.py\", line 1080, in __init__\n    self.run()\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2817, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2843, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3018, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3183, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3265, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-60-9779c50287bd>\", line 2, in <module>\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=target_data, logits=logits))\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1888, in softmax_cross_entropy_with_logits_v2\n    precise_logits, labels, name=name)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 7747, in softmax_cross_entropy_with_logits\n    name=name)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3272, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Sophia\\Anaconda3\\envs\\tfcourse\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1768, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): logits and labels must be broadcastable: logits_size=[2560,2596] labels_size=[128,2596]\n\t [[{{node softmax_cross_entropy_with_logits}} = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](softmax_cross_entropy_with_logits/Reshape, IteratorGetNext:1)]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # wir starten damit die Variablen zu initialisieren\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # bestimme die Anzahl der Epochen\n",
    "    for epoch in range(100):\n",
    "    \n",
    "        # initialisiere einen Step counter um an den richtigen Stellen die Placeholder einzuspeisen\n",
    "        global_step = 0\n",
    "        # lade das Datenset in den Iterator\n",
    "        sess.run(iterator_init_op)\n",
    "\n",
    "        # gehe durch das Datenset bis es leer ist\n",
    "        while True:\n",
    "            try:\n",
    "\n",
    "                # Starte das Training und gebe den loss aus\n",
    "                # au√üerdem gebe die states der beiden LSTM Layer aus um sie f√ºr den n√§chsten Batch zu benutzen\n",
    "                loss_val, _ = sess.run((loss, training_step))\n",
    "                \n",
    "                # Erh√∂he den Global Step Count um 1\n",
    "                global_step += 1\n",
    "\n",
    "            # Stoppt sobald der Iterator leer ist\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "                \n",
    "       # Nach jeder Epoche soll der loss ausgegeben werden um den Trainingsprozess zu √ºberwachen\n",
    "        print(\"Epoch: {}, Loss: {:f}\".format(epoch, loss_val))\n",
    "        \n",
    "        # Wir wollen schon w√§hrend des Trainings Tweets generieren um zu schauen, wie sich die Qualit√§t ver√§ndert\n",
    "        \n",
    "        # Bestimme die L√§nge der generierten Tweets\n",
    "        tweet_length = 20\n",
    "        \n",
    "        # W√§hle zuf√§llig eine Startsequenz aus\n",
    "        start_idx = random.randint(0, len(text_idx) - seq_len)\n",
    "        seq_idx = text_idx[start_idx:start_idx + seq_len]      \n",
    "      \n",
    "        # Erstelle eine leere Liste um die generierten W√∂rter abzuspeichern\n",
    "        sample_seq_idx = []\n",
    "        \n",
    "        # Generiere Wort f√ºr Wort einen euen Tweet\n",
    "        for n in range(tweet_length):\n",
    "            \n",
    "            # Da wir die generierten Tweets nur anschauen wollen und nicht mit einem Target vergleichen, brauchen wir Faketargets\n",
    "            fake_target = np.zeros([1, vocab_size], dtype=np.int32)\n",
    "            sample_dataset = tf.data.Dataset.from_tensor_slices(([seq_idx], fake_target))\n",
    "            # Lade das Sample Datenset in den Iterator\n",
    "            sess.run(iterator.make_initializer(sample_dataset))\n",
    "    \n",
    "            # Lese den Softmax Output der letzten Layer aus (dieser liefert die Wahrscheinlichkeiten f√ºr das n√§chste generierte Wort)\n",
    "            # und gebe den state der LSTM Layer immer wieder neu rein\n",
    "            sample_output = sess.run((logits))\n",
    "\n",
    "            # W√§hle zuf√§llig ein Wort aus der Softmax Distribution und h√§nge es an die Liste an\n",
    "            sample = np.random.choice(range(vocab_size), p=sample_output.ravel())\n",
    "            sample_seq_idx.append(sample)\n",
    "            # Update die Startsequenz mit dem neuen Wort um das n√§chste Wort zu generieren\n",
    "            seq_idx = seq_idx[1:] + [sample]\n",
    "        \n",
    "      \n",
    "        # Zeige den generierten Tweets an\n",
    "        # da wir in dieser Tokenisierung keine Leerzeichen haben, muss zwischen jedes Wort eins gesetzt werden\n",
    "        sample_txt = ' '.join(idx_to_char[idx] for idx in sample_seq_idx)\n",
    "        print('----\\n %s \\n----\\n' % (sample_txt,))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
